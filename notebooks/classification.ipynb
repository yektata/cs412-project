{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yektata/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yektata/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "/var/folders/sx/n3yd3gln07zgzs8d9fmrbbmr0000gn/T/ipykernel_80428/253959560.py:25: DeprecationWarning: invalid escape sequence '\\B'\n",
      "  text = text.encode('utf-8').decode('unicode-escape')\n",
      "/var/folders/sx/n3yd3gln07zgzs8d9fmrbbmr0000gn/T/ipykernel_80428/253959560.py:25: DeprecationWarning: invalid escape sequence '\\M'\n",
      "  text = text.encode('utf-8').decode('unicode-escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 94043 entries\n",
      "\n",
      "First few processed texts:\n",
      "\n",
      "Original: cumhuriyetimizin 100 yili kutlu olsunï ð 1ð\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/yektata/nltk_data'\n    - '/Users/yektata/VSCodeProjects/412project/.venv/nltk_data'\n    - '/Users/yektata/VSCodeProjects/412project/.venv/share/nltk_data'\n    - '/Users/yektata/VSCodeProjects/412project/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m processed_texts[:\u001b[38;5;241m3\u001b[39m]:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtokenize_turkish_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Create TF-IDF matrix\u001b[39;00m\n\u001b[1;32m    105\u001b[0m vectorizer, tfidf_matrix \u001b[38;5;241m=\u001b[39m create_tfidf_matrix(processed_texts)\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mtokenize_turkish_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_turkish_text\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Tokenize Turkish text using NLTK\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/yektata/nltk_data'\n    - '/Users/yektata/VSCodeProjects/412project/.venv/nltk_data'\n    - '/Users/yektata/VSCodeProjects/412project/.venv/share/nltk_data'\n    - '/Users/yektata/VSCodeProjects/412project/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Union\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "try:\n",
    "    nltk.download('stopwords')\n",
    "except:\n",
    "    print(\"Note: Turkish stopwords might not be available in NLTK\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text by removing Unicode escape sequences and normalizing Unicode characters\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Decode Unicode escape sequences\n",
    "    text = text.encode('utf-8').decode('unicode-escape')\n",
    "    \n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Remove special characters and extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "def process_json_entry(entry: Dict[str, Union[str, int, float, List]]) -> str:\n",
    "    \"\"\"\n",
    "    Extract and clean text fields from a JSON entry\n",
    "    \"\"\"\n",
    "    # Fields to process - add more if needed\n",
    "    text_fields = ['clean_caption', 'biography']\n",
    "    \n",
    "    # Combine all text fields\n",
    "    combined_text = ' '.join(str(entry.get(field, '')) for field in text_fields)\n",
    "    \n",
    "    return clean_text(combined_text)\n",
    "\n",
    "def tokenize_turkish_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize Turkish text using NLTK\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def process_jsonl_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Process JSONL file and return cleaned, tokenized texts\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    entry = json.loads(line.strip())\n",
    "                    cleaned_text = process_json_entry(entry)\n",
    "                    if cleaned_text:  # Only add non-empty texts\n",
    "                        processed_texts.append(cleaned_text)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON line: {line[:50]}...\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "    return processed_texts\n",
    "\n",
    "def create_tfidf_matrix(texts: List[str]):\n",
    "    \"\"\"\n",
    "    Create TF-IDF matrix from processed texts\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenize_turkish_text,\n",
    "        lowercase=True,\n",
    "        max_features=5000  # Adjust as needed\n",
    "    )\n",
    "    \n",
    "    return vectorizer, vectorizer.fit_transform(texts)\n",
    "\n",
    "# Replace with your file path\n",
    "file_path = \"processed_dataset-r3v9.jsonl\"\n",
    "\n",
    "# Process the file\n",
    "processed_texts = process_jsonl_file(file_path)\n",
    "\n",
    "if processed_texts:\n",
    "    print(f\"Processed {len(processed_texts)} entries\")\n",
    "    \n",
    "    # Example of processing first few texts\n",
    "    print(\"\\nFirst few processed texts:\")\n",
    "    for text in processed_texts[:3]:\n",
    "        print(\"\\nOriginal:\", text)\n",
    "        print(\"Tokenized:\", tokenize_turkish_text(text))\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    vectorizer, tfidf_matrix = create_tfidf_matrix(processed_texts)\n",
    "    print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "    \n",
    "    # Example of getting feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nFirst 10 features: {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yektata/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yektata/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yektata/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 lines...\n",
      "Processed 20000 lines...\n",
      "Processed 30000 lines...\n",
      "Processed 40000 lines...\n",
      "Processed 50000 lines...\n",
      "Processed 60000 lines...\n",
      "Processed 70000 lines...\n",
      "Processed 80000 lines...\n",
      "Processed 90000 lines...\n",
      "Finished processing with 0 errors\n",
      "\n",
      "Processed 93927 entries\n",
      "\n",
      "First few processed texts:\n",
      "\n",
      "Original: cumhuriyetimizin 100 yili kutlu olsun\n",
      "Tokenized: ['cumhuriyetimizin', '100', 'yili', 'kutlu', 'olsun']\n",
      "\n",
      "Original: oriflame duologi lansmani\n",
      "Tokenized: ['oriflame', 'duologi', 'lansmani']\n",
      "\n",
      "Original: 07agustos 23 oriflameturkiye\n",
      "Tokenized: ['07agustos', '23', 'oriflameturkiye']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yektata/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF matrix shape: (93927, 5000)\n",
      "\n",
      "First 10 features: ['0' '00' '000' '0090' '01' '012' '02' '0212' '0216' '0224']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Union\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "try:\n",
    "    nltk.download('stopwords')\n",
    "except:\n",
    "    print(\"Note: Turkish stopwords might not be available in NLTK\")\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced text cleaning function with better Unicode handling\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace common problematic Unicode characters\n",
    "    text = text.replace('\\\\u2019', \"'\")  # Smart quote\n",
    "    text = text.replace('\\\\u2018', \"'\")  # Smart quote\n",
    "    text = text.replace('\\\\u201c', '\"')  # Smart quote\n",
    "    text = text.replace('\\\\u201d', '\"')  # Smart quote\n",
    "    \n",
    "    # Handle emoji and other special characters\n",
    "    text = re.sub(r'\\\\[uU][0-9a-fA-F]{4}', ' ', text)  # Remove Unicode escape sequences\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    \n",
    "    # Normalize remaining Unicode\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Remove special characters but keep Turkish letters\n",
    "    text = re.sub(r'[^a-zA-Z0-9ğüşıöçĞÜŞİÖÇ\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "def process_json_entry(entry: Dict[str, Union[str, int, float, List]]) -> str:\n",
    "    \"\"\"\n",
    "    Extract and clean text fields from a JSON entry\n",
    "    \"\"\"\n",
    "    # Fields to process\n",
    "    text_fields = ['clean_caption', 'biography']\n",
    "    \n",
    "    # Combine all text fields with proper handling of None values\n",
    "    combined_text = ' '.join(str(entry.get(field, '')) for field in text_fields if entry.get(field))\n",
    "    \n",
    "    return clean_text(combined_text)\n",
    "\n",
    "def tokenize_turkish_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize Turkish text using NLTK\n",
    "    \"\"\"\n",
    "    # Additional preprocessing for Turkish-specific cases\n",
    "    text = text.replace('i̇', 'i')  # Handle dotted i issues\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def process_jsonl_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Process JSONL file and return cleaned, tokenized texts\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    error_count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                try:\n",
    "                    entry = json.loads(line.strip())\n",
    "                    cleaned_text = process_json_entry(entry)\n",
    "                    if cleaned_text:  # Only add non-empty texts\n",
    "                        processed_texts.append(cleaned_text)\n",
    "                except json.JSONDecodeError:\n",
    "                    error_count += 1\n",
    "                    print(f\"Error parsing JSON at line {line_num}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    print(f\"Error processing line {line_num}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Print progress every 10000 lines\n",
    "                if line_num % 10000 == 0:\n",
    "                    print(f\"Processed {line_num} lines...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Finished processing with {error_count} errors\")\n",
    "    return processed_texts\n",
    "\n",
    "def create_tfidf_matrix(texts: List[str]):\n",
    "    \"\"\"\n",
    "    Create TF-IDF matrix from processed texts\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenize_turkish_text,\n",
    "        lowercase=True,\n",
    "        max_features=5000,  # Adjust as needed\n",
    "        min_df=5,  # Minimum document frequency\n",
    "        max_df=0.95  # Maximum document frequency\n",
    "    )\n",
    "    \n",
    "    return vectorizer, vectorizer.fit_transform(texts)\n",
    "\n",
    "# Replace with your file path\n",
    "file_path = \"processed_dataset-r3v9.jsonl\"\n",
    "\n",
    "# Process the file\n",
    "processed_texts = process_jsonl_file(file_path)\n",
    "\n",
    "if processed_texts:\n",
    "    print(f\"\\nProcessed {len(processed_texts)} entries\")\n",
    "    \n",
    "    # Example of processing first few texts\n",
    "    print(\"\\nFirst few processed texts:\")\n",
    "    for text in processed_texts[:3]:\n",
    "        print(\"\\nOriginal:\", text)\n",
    "        print(\"Tokenized:\", tokenize_turkish_text(text))\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    vectorizer, tfidf_matrix = create_tfidf_matrix(processed_texts)\n",
    "    print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "    \n",
    "    # Example of getting feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nFirst 10 features: {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 features: ['0' '00' '000' '0090' '01' '012' '02' '0212' '0216' '0224' '0232' '0242'\n",
      " '0252' '0262' '0264' '0266' '03' '0312' '0332' '035' '0352' '0380' '04'\n",
      " '0422' '05' '0505' '0506' '0507' '0530' '0531' '0532' '0533' '0534'\n",
      " '0535' '0536' '0537' '0538' '0539' '0541' '0542' '0543' '0544' '0545'\n",
      " '0546' '0549' '0552' '0553' '0554' '0555' '06' '07' '08' '0850' '09' '1'\n",
      " '10' '100' '1000' '100th' '101' '105' '11' '12' '120' '125' '13' '137'\n",
      " '14' '140' '15' '150' '1500' '153' '154' '16' '17' '170' '18' '180' '19'\n",
      " '1954' '1970' '1978' '1986' '1988' '1994' '1995' '2' '20' '200' '2000'\n",
      " '2001' '2002' '2005' '2012' '2014' '2015' '2018' '2019' '2020' '2021'\n",
      " '2022' '2023' '2024' '208' '21' '210' '212' '214' '216' '22' '220' '222'\n",
      " '223' '224' '23' '230' '232' '234' '236' '24' '241' '242' '243' '246'\n",
      " '25' '250' '252' '255' '256' '258' '259' '26' '262' '264' '265' '266'\n",
      " '27' '272' '28' '285' '286' '287' '29' '291' '3' '30' '300' '302' '304'\n",
      " '31' '311' '312' '317' '318' '32' '321' '322' '323' '33' '333' '335' '34'\n",
      " '342' '343' '346' '348' '349' '35' '350' '352' '354' '358' '36' '360'\n",
      " '37' '38' '380' '381' '385' '388' '39' '395' '397' '3d' '4' '40' '400'\n",
      " '401' '41' '412' '413' '42' '426' '43' '44' '441' '442' '444' '45' '46'\n",
      " '462' '464' '465' '47' '48' '49' '494' '5' '50' '500' '502' '503' '505'\n",
      " '51' '51digital' '52' '53' '530' '531' '532' '533' '534' '535' '537'\n",
      " '538' '539' '54' '541' '542' '543' '544' '549' '55' '552' '553' '554'\n",
      " '555' '56' '567' '57' '572' '58' '59' '6' '60' '600' '61' '62' '63' '631'\n",
      " '64' '65' '651' '66' '67' '68' '686' '69' '7' '70' '700' '71' '716' '718'\n",
      " '72' '73' '74' '75' '76' '77' '777' '778' '78' '784' '79' '8' '80' '800'\n",
      " '81' '82' '83' '84' '840' '85' '850' '86' '87' '88' '888' '89' '896' '9'\n",
      " '90' '90532' '91' '92' '926' '93' '94' '95' '96' '97' '98' '99' 'a'\n",
      " 'abdullah' 'abone' 'about' 'ac' 'acaba' 'academy' 'acan' 'acar'\n",
      " 'accommodation' 'account' 'acele' 'aci' 'acibadem' 'acigiz' 'acik'\n",
      " 'acikhava' 'acil' 'acilan' 'acildi' 'acilis' 'acin' 'acisindan' 'actress'\n",
      " 'ada' 'adam' 'adana' 'adapazari' 'adasi' 'aday' 'adayi' 'add' 'adet'\n",
      " 'adeta' 'adi' 'adidas' 'adil' 'adim' 'adimda' 'adimi' 'adimlar' 'adina'\n",
      " 'adini' 'adli' 'adres' 'adresi' 'adresinden' 'adresini' 'ads' 'adventure'\n",
      " 'aegean' 'aesthetic' 'afet' 'afiyet' 'afiyetle' 'after' 'ag' 'agac'\n",
      " 'again' 'agency' 'agi' 'agir' 'agirlamaktan' 'agiz' 'agri' 'agustos'\n",
      " 'ahmet' 'ahsap' 'ai' 'aile' 'ailesi' 'ailesine' 'ailesinin' 'air' 'ait'\n",
      " 'aittir' 'ajansi' 'ak' 'akademi' 'akademik' 'akademisi' 'akasya'\n",
      " 'akdeniz' 'akil' 'akilli' 'akmerkez' 'aks' 'aksam' 'aksami' 'aksaray'\n",
      " 'aksesuar' 'aksesuarlari' 'aktif' 'aktivite' 'al' 'alabilirsiniz'\n",
      " 'alacak' 'alacati' 'alalim' 'alan' 'alana' 'alanda' 'alani' 'alanina'\n",
      " 'alaninda' 'alanlar' 'alanlarda' 'alanlari' 'alanlarinda' 'alanya'\n",
      " 'alarak' 'albeni' 'aldi' 'aldigi' 'aldigimiz' 'aldik' 'aldim' 'aletleri'\n",
      " 'ali' 'alici' 'alin' 'alinan' 'alip' 'alir' 'alirken' 'alis' 'alisveris'\n",
      " 'alisverise' 'alisverisin' 'alisverislerin' 'aliyor' 'aliyoruz' 'alkol'\n",
      " 'all' 'allah' 'alma' 'almak' 'alman' 'almanya' 'almaya' 'almis' 'alo'\n",
      " 'alsancak' 'also' 'alt' 'alternatif' 'alti' 'altin' 'altina' 'altinda'\n",
      " 'altinyayla' 'always' 'am' 'ama' 'amaci' 'amaciyla' 'amacli' 'amaclidir'\n",
      " 'amazing' 'amazon' 'ambalaj' 'ambassador' 'ameliyat' 'amerika'\n",
      " 'amsterdam' 'an' 'ana' 'anadolu' 'anahtar' 'anahtari' 'analiz' 'analizi'\n",
      " 'anaokulu' 'anayurt' 'ancak' 'and' 'anda' 'ani' 'anilar' 'anin' 'aninda'\n",
      " 'aniyor' 'aniyorum' 'aniyoruz' 'ankara' 'anlamda' 'anlamina' 'anlamli'\n",
      " 'anlar' 'anlari' 'anlatiyor' 'anlayisi' 'anma' 'anne' 'anneler'\n",
      " 'annelerimizin' 'annem' 'annesi' 'anniversary' 'another' 'antakya'\n",
      " 'antalya' 'antep' 'antik' 'antrenman' 'any' 'app' 'apple' 'application'\n",
      " 'ar' 'ara' 'arac' 'araciligiyla' 'arada' 'aradiginiz' 'aralik' 'arama'\n",
      " 'aramiza' 'aramizdan' 'aras' 'arasi' 'arasina' 'arasinda' 'arasindaki'\n",
      " 'arastirma' 'araya' 'arayabilirsiniz' 'arayin' 'architecture' 'arda'\n",
      " 'ardindan' 'are' 'arena' 'ari' 'ariyoruz' 'arka' 'arkadas' 'arkadasim'\n",
      " 'arkadasini' 'arkadaslar' 'arkadaslari' 'arkadaslarimiz'\n",
      " 'arkadaslarimiza' 'arkadaslarini' 'arkasi' 'arkasinda' 'armagan' 'aroma'\n",
      " 'around' 'ars' 'arslan' 'art' 'artan' 'artik' 'artirin' 'artirir'\n",
      " 'artirmak' 'artist' 'arts' 'as' 'asagidaki' 'asi' 'asil' 'asiri'\n",
      " 'asirlik' 'asistani' 'asit' 'ask' 'askin' 'asla' 'aslan' 'aslinda'\n",
      " 'aspat' 'association' 'asya' 'at' 'ata' 'atabilirsiniz' 'atakent'\n",
      " 'atakoy' 'atan' 'atasehir' 'ataturk' 'ates' 'ateste' 'athlete' 'atik'\n",
      " 'atin' 'atiyor' 'atmak' 'atmosphere' 'ato' 'atolye' 'atolyeleri'\n",
      " 'atolyesi' 'audi' 'author' 'av' 'available' 'avantajli' 'avcilar'\n",
      " 'aviation' 'avm' 'avrasya' 'avrupa' 'avuc' 'awaits' 'award' 'awards' 'ay'\n",
      " 'ayak' 'ayakkabi' 'ayar' 'ayda' 'aydin' 'aydinlatma' 'aydinlik' 'ayi'\n",
      " 'ayin' 'ayinda' 'ayinin' 'aylarinda' 'aylik' 'ayni' 'ayri' 'ayrica'\n",
      " 'ayricalikli' 'ayrilisinin' 'ayrintili' 'ayse' 'ayvalik' 'az' 'azaltir'\n",
      " 'azerbaycan' 'aziz' 'b' 'baba' 'babalar' 'babasi' 'baby' 'back' 'badem'\n",
      " 'bag' 'bagdat' 'bagimsiz' 'bagimsizlik' 'bagirsak' 'bagis' 'bagisiklik'\n",
      " 'baglanti' 'bagli' 'bahar' 'baharat' 'bahc' 'bahce' 'bahcelievler'\n",
      " 'bahcesehir' 'bak' 'bakalim' 'bakan' 'bakani' 'bakanimiz' 'bakanligi'\n",
      " 'bakim' 'bakimi' 'bakin' 'bakis' 'baklava' 'baku' 'bal' 'balik'\n",
      " 'balikesir' 'balkon' 'bambaska' 'bana' 'bandirma' 'banka' 'bankasi'\n",
      " 'banu' 'banyo' 'bar' 'bardagi' 'bardak' 'barindiran' 'baris' 'bas'\n",
      " 'basari' 'basarilar' 'basarilarinin' 'basarili' 'basariya' 'basariyla'\n",
      " 'based' 'basi' 'basin' 'basina' 'basinda' 'basit' 'baska' 'baskan'\n",
      " 'baskani' 'baskanimiz' 'baskanligi' 'basketbol' 'baski' 'basla' 'basladi'\n",
      " 'basladik' 'baslamak' 'baslamistir' 'baslangic' 'baslar' 'baslasin'\n",
      " 'baslayacak' 'baslayan' 'baslayin' 'baslikli' 'basliyor' 'basliyoruz'\n",
      " 'bassagligi' 'basta' 'basvuru' 'basvurunuz' 'bati' 'batman' 'bay' 'bayan'\n",
      " 'bayi' 'bayrak' 'bayrakli' 'bayram' 'bayrami' 'bayramimiz' 'bayraminiz'\n",
      " 'bayramlar' 'bazen' 'bazi' 'bazli' 'be' 'beach' 'beautiful' 'beauty'\n",
      " 'bebeginizin' 'bebek' 'bebekler' 'beden' 'been' 'before' 'begen' 'begeni'\n",
      " 'begenmeyi' 'being' 'beklenen' 'bekleriz' 'beklet29' 'bekleyen'\n",
      " 'bekliyor' 'bekliyorum' 'bekliyoruz' 'bel' 'belediye' 'belediyemiz'\n",
      " 'belediyesi' 'belirli' 'belki' 'belli' 'ben' 'bence' 'benden' 'beni'\n",
      " 'benim' 'benimle' 'benzer' 'benzeri' 'benzersiz' 'beraber' 'bereket'\n",
      " 'bereketli' 'beri' 'berk' 'berlin' 'bes' 'besiktas' 'besin' 'beslenme'\n",
      " 'best' 'beta' 'better' 'between' 'bey' 'beyaz' 'beyin' 'beykent' 'beykoz'\n",
      " 'beylikduzu' 'beymen' 'beyoglu' 'beyond' 'bi' 'biber' 'big' 'bigbosslayf'\n",
      " 'bildiginiz' 'bile' 'bilen' 'bilet' 'bileti' 'biletinial' 'biletix'\n",
      " 'biletler' 'bilgi' 'bilgilendirme' 'bilgiler' 'bilgileri' 'bilgisayar'\n",
      " 'bilgisi' 'bilgiye' 'bilim' 'bilimsel' 'bilincli' 'bilinen' 'bilir'\n",
      " 'bilisim' 'biliyor' 'biliyoruz' 'bilkent' 'bin' 'bina' 'binasi'\n",
      " 'binlerce' 'bio' 'bir' 'birak' 'birakan' 'birakin' 'birakmak' 'biraktik'\n",
      " 'biraz' 'birbirinden' 'birbirine' 'bircok' 'birden' 'birebir' 'birer'\n",
      " 'birey' 'bireyler' 'bireysel' 'biri' 'biridir' 'birinci' 'birinde'\n",
      " 'birini' 'birisi' 'birkac' 'birligi' 'birlik' 'birlikleri' 'birlikte'\n",
      " 'bisiklet' 'bit' 'bitki' 'bitkisel' 'bitmeyen' 'bitmez' 'bitti'\n",
      " 'biyolojik' 'biz' 'bizde' 'bizden' 'bize' 'bizi' 'bizim' 'bizimle'\n",
      " 'bizler' 'bizlere' 'bizleri' 'bizlerle' 'black' 'blockchain' 'blog'\n",
      " 'blogger' 'blok' 'blue' 'blv' 'bmw' 'board' 'boat' 'bobrek' 'bodrum'\n",
      " 'body' 'bogaz' 'bogazici' 'bol' 'bolca' 'bolge' 'bolgenin' 'bolgesel'\n",
      " 'bolgesi' 'bolgesinde' 'bolu' 'bolum' 'bolumu' 'bomonti' 'bonus' 'book'\n",
      " 'booking' 'booth' 'bornova' 'bos' 'bosphorus' 'both' 'boy' 'boya' 'boyle'\n",
      " 'boylece' 'boyu' 'boyun' 'boyunca' 'brand' 'brands' 'breakfast' 'breath'\n",
      " 'bring' 'bsk' 'bu' 'bubble' 'bubilet' 'bufe' 'bugday' 'bugu' 'bugun'\n",
      " 'bugune' 'bugunku' 'bugunun' 'building' 'bulabilirsiniz' 'bulunan'\n",
      " 'bulundu' 'bulundugu' 'bulunduk' 'bulunmaktadir' 'bulunur' 'bulunuyor'\n",
      " 'bulusalim' 'bulusma' 'bulusmak' 'bulusmasi' 'bulusmaya' 'bulustu'\n",
      " 'bulustugu' 'bulusturan' 'bulusturuyor' 'bulusturuyoruz' 'bulusuyor'\n",
      " 'bulusuyoruz' 'bulut' 'bulvar' 'bulvari' 'buna' 'bundan' 'bunlar'\n",
      " 'bunlari' 'bunlarin' 'bunu' 'bunun' 'burada' 'buradan' 'buradayiz'\n",
      " 'burak' 'burasi' 'buraya' 'burda' 'burdur' 'burger' 'bursa' 'bursaspor'\n",
      " 'burun' 'business' 'but' 'butce' 'butik' 'butun' 'buyuk' 'buyukcekmece'\n",
      " 'buyuksehir' 'buyuleyici' 'buyulu' 'buz' 'by' 'c' 'cad' 'cadde' 'caddesi'\n",
      " 'caeli' 'cafe']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFirst 10 features: {feature_names[:1000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text data...\n",
      "Processed 10000 lines...\n",
      "Processed 20000 lines...\n",
      "Processed 30000 lines...\n",
      "Processed 40000 lines...\n",
      "Processed 50000 lines...\n",
      "Processed 60000 lines...\n",
      "Processed 70000 lines...\n",
      "Processed 80000 lines...\n",
      "Processed 90000 lines...\n",
      "Finished processing with 0 errors\n",
      "\n",
      "Total samples: 93927\n",
      "Number of classes: 10\n",
      "\n",
      "Creating TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yektata/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest model...\n",
      "\n",
      "Results:\n",
      "Training Accuracy: 0.9914\n",
      "Validation Accuracy: 0.8929\n",
      "Training F1 Score: 0.9914\n",
      "Validation F1 Score: 0.8929\n",
      "\n",
      "Top 10 most important features:\n",
      "lezzet: 0.0098\n",
      "ve: 0.0071\n",
      "teknoloji: 0.0065\n",
      "com: 0.0059\n",
      "rezervasyon: 0.0047\n",
      "siparis: 0.0045\n",
      "lezzetli: 0.0045\n",
      "icin: 0.0043\n",
      "belediye: 0.0040\n",
      "baskani: 0.0038\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Union\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Previous functions remain the same until create_tfidf_matrix\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.replace('\\\\u2019', \"'\")\n",
    "    text = text.replace('\\\\u2018', \"'\")\n",
    "    text = text.replace('\\\\u201c', '\"')\n",
    "    text = text.replace('\\\\u201d', '\"')\n",
    "    \n",
    "    text = re.sub(r'\\\\[uU][0-9a-fA-F]{4}', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z0-9ğüşıöçĞÜŞİÖÇ\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "def process_json_entry(entry: Dict[str, Union[str, int, float, List]]) -> str:\n",
    "    text_fields = ['clean_caption', 'biography']\n",
    "    combined_text = ' '.join(str(entry.get(field, '')) for field in text_fields if entry.get(field))\n",
    "    return clean_text(combined_text)\n",
    "\n",
    "def tokenize_turkish_text(text: str) -> List[str]:\n",
    "    text = text.replace('i̇', 'i')\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def process_jsonl_file(file_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Process JSONL file and return cleaned texts and labels\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    labels = []\n",
    "    error_count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                try:\n",
    "                    entry = json.loads(line.strip())\n",
    "                    cleaned_text = process_json_entry(entry)\n",
    "                    if cleaned_text:\n",
    "                        processed_texts.append(cleaned_text)\n",
    "                        # Get the label (train_category)\n",
    "                        labels.append(entry.get('train_category', 'unknown'))\n",
    "                except json.JSONDecodeError:\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "                \n",
    "                if line_num % 10000 == 0:\n",
    "                    print(f\"Processed {line_num} lines...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        return [], []\n",
    "    \n",
    "    print(f\"Finished processing with {error_count} errors\")\n",
    "    return processed_texts, labels\n",
    "\n",
    "def create_tfidf_matrix(texts: List[str]):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenize_turkish_text,\n",
    "        lowercase=True,\n",
    "        max_features=5000,\n",
    "        min_df=5,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    return vectorizer, vectorizer.fit_transform(texts)\n",
    "\n",
    "def train_and_evaluate_random_forest(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate Random Forest model\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_preds = rf_model.predict(X_train)\n",
    "    test_preds = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    # Calculate F1 scores (micro for multi-class)\n",
    "    train_f1 = f1_score(y_train, train_preds, average='micro')\n",
    "    test_f1 = f1_score(y_test, test_preds, average='micro')\n",
    "    \n",
    "    return {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_f1': train_f1,\n",
    "        'test_f1': test_f1,\n",
    "        'model': rf_model,\n",
    "        'feature_importance': rf_model.feature_importances_\n",
    "    }\n",
    "\n",
    "# Replace with your file path\n",
    "file_path = \"processed_dataset-r3v10.jsonl\"\n",
    "\n",
    "# Process the file\n",
    "print(\"Processing text data...\")\n",
    "processed_texts, labels = process_jsonl_file(file_path)\n",
    "\n",
    "if processed_texts:\n",
    "    print(f\"\\nTotal samples: {len(processed_texts)}\")\n",
    "    print(f\"Number of classes: {len(set(labels))}\")\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    print(\"\\nCreating TF-IDF matrix...\")\n",
    "    vectorizer, tfidf_matrix = create_tfidf_matrix(processed_texts)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    results = train_and_evaluate_random_forest(tfidf_matrix, labels)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Training Accuracy: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Training F1 Score: {results['train_f1']:.4f}\")\n",
    "    print(f\"Validation F1 Score: {results['test_f1']:.4f}\")\n",
    "    \n",
    "    # Print top features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_features_idx = np.argsort(results['feature_importance'])[-10:]\n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    for idx in top_features_idx[::-1]:\n",
    "        print(f\"{feature_names[idx]}: {results['feature_importance'][idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib/python3.13/site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.13/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.venv/lib/python3.13/site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processed 10000 lines...\n",
      "Processed 20000 lines...\n",
      "Processed 30000 lines...\n",
      "Processed 40000 lines...\n",
      "Processed 50000 lines...\n",
      "Processed 60000 lines...\n",
      "Processed 70000 lines...\n",
      "Processed 80000 lines...\n",
      "Processed 90000 lines...\n",
      "Finished processing with 0 errors\n",
      "\n",
      "Total training samples: 93927\n",
      "Number of classes: 10\n",
      "\n",
      "Creating TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yektata/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest model...\n",
      "\n",
      "Results:\n",
      "Training Accuracy: 0.9914\n",
      "Validation Accuracy: 0.8929\n",
      "Training F1 Score: 0.9914\n",
      "Validation F1 Score: 0.8929\n",
      "\n",
      "Predicting test data...\n",
      "\n",
      "Saving predictions...\n",
      "\n",
      "Predictions saved to 'prediction-classification-round3.json'\n",
      "Confusion matrix saved as 'confusion_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Union\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# [Previous functions remain the same until train_and_evaluate_random_forest]\n",
    "\n",
    "def train_and_evaluate_random_forest(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate Random Forest model with confusion matrix\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_preds = rf_model.predict(X_train)\n",
    "    test_preds = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    train_f1 = f1_score(y_train, train_preds, average='micro')\n",
    "    test_f1 = f1_score(y_test, test_preds, average='micro')\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, test_preds)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Get unique labels for axis labels\n",
    "    unique_labels = sorted(set(y))\n",
    "    plt.xticks(np.arange(len(unique_labels)) + 0.5, unique_labels, rotation=45)\n",
    "    plt.yticks(np.arange(len(unique_labels)) + 0.5, unique_labels, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_f1': train_f1,\n",
    "        'test_f1': test_f1,\n",
    "        'model': rf_model,\n",
    "        'feature_importance': rf_model.feature_importances_,\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': unique_labels\n",
    "    }\n",
    "\n",
    "def predict_test_data(model, vectorizer, test_file_path: str):\n",
    "    \"\"\"\n",
    "    Predict categories for test data and aggregate by username\n",
    "    \"\"\"\n",
    "    username_predictions = {}\n",
    "    username_texts = {}\n",
    "    \n",
    "    # Read and process test data\n",
    "    with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                username = entry.get('username')\n",
    "                if not username:\n",
    "                    continue\n",
    "                \n",
    "                # Process text\n",
    "                text = process_json_entry(entry)\n",
    "                if text:\n",
    "                    if username not in username_texts:\n",
    "                        username_texts[username] = []\n",
    "                    username_texts[username].append(text)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Make predictions for each username\n",
    "    for username, texts in username_texts.items():\n",
    "        # Transform texts to TF-IDF\n",
    "        text_features = vectorizer.transform(texts)\n",
    "        \n",
    "        # Get predictions for all posts\n",
    "        predictions = model.predict(text_features)\n",
    "        \n",
    "        # Count predictions and handle ties\n",
    "        pred_counter = Counter(predictions)\n",
    "        max_count = max(pred_counter.values())\n",
    "        \n",
    "        # Get all labels with maximum count\n",
    "        max_labels = [label for label, count in pred_counter.items() \n",
    "                     if count == max_count]\n",
    "        \n",
    "        if len(max_labels) == 1:\n",
    "            # Clear winner\n",
    "            final_prediction = max_labels[0]\n",
    "        else:\n",
    "            # Tie-breaking: use the label with highest confidence scores\n",
    "            confidences = []\n",
    "            for label in max_labels:\n",
    "                # Get indices where this label was predicted\n",
    "                label_indices = [i for i, pred in enumerate(predictions) if pred == label]\n",
    "                # Get average confidence for this label\n",
    "                label_confidences = model.predict_proba(text_features[label_indices])\n",
    "                avg_confidence = np.mean([conf[list(model.classes_).index(label)] \n",
    "                                       for conf in label_confidences])\n",
    "                confidences.append((label, avg_confidence))\n",
    "            \n",
    "            # Choose label with highest average confidence\n",
    "            final_prediction = max(confidences, key=lambda x: x[1])[0]\n",
    "        \n",
    "        username_predictions[username] = final_prediction\n",
    "    \n",
    "    return username_predictions\n",
    "\n",
    "# Training phase\n",
    "train_file_path = \"processed_dataset-r3v10.jsonl\"\n",
    "test_file_path = \"processed_dataset-r3v10-test.jsonl\"\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "processed_texts, labels = process_jsonl_file(train_file_path)\n",
    "\n",
    "if processed_texts:\n",
    "    print(f\"\\nTotal training samples: {len(processed_texts)}\")\n",
    "    print(f\"Number of classes: {len(set(labels))}\")\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    print(\"\\nCreating TF-IDF matrix...\")\n",
    "    vectorizer, tfidf_matrix = create_tfidf_matrix(processed_texts)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    results = train_and_evaluate_random_forest(tfidf_matrix, labels)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Training Accuracy: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Training F1 Score: {results['train_f1']:.4f}\")\n",
    "    print(f\"Validation F1 Score: {results['test_f1']:.4f}\")\n",
    "    \n",
    "    # Predict test data\n",
    "    print(\"\\nPredicting test data...\")\n",
    "    predictions = predict_test_data(\n",
    "        results['model'],\n",
    "        vectorizer,\n",
    "        test_file_path\n",
    "    )\n",
    "    \n",
    "    # Save predictions to file\n",
    "    print(\"\\nSaving predictions...\")\n",
    "    with open('prediction-classification-round3.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"\\nPredictions saved to 'prediction-classification-round3.json'\")\n",
    "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processed 10000 lines...\n",
      "Processed 20000 lines...\n",
      "Processed 30000 lines...\n",
      "Processed 40000 lines...\n",
      "Processed 50000 lines...\n",
      "Processed 60000 lines...\n",
      "Processed 70000 lines...\n",
      "Processed 80000 lines...\n",
      "Processed 90000 lines...\n",
      "Finished processing with 0 errors\n",
      "\n",
      "Calculating class weights...\n",
      "\n",
      "Class distribution weights:\n",
      "tech: 0.796\n",
      "food: 0.538\n",
      "health and lifestyle: 0.538\n",
      "travel: 0.944\n",
      "sports: 2.432\n",
      "fashion: 0.909\n",
      "entertainment: 0.860\n",
      "mom and children: 1.815\n",
      "art: 1.442\n",
      "gaming: 20.643\n",
      "\n",
      "Creating TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yektata/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest model...\n",
      "\n",
      "Results:\n",
      "Training Accuracy: 0.9914\n",
      "Validation Accuracy: 0.8929\n",
      "Training F1 Score: 0.9914\n",
      "Validation F1 Score: 0.8929\n",
      "\n",
      "Predicting test data with balanced voting...\n",
      "\n",
      "Saving predictions...\n",
      "\n",
      "Predictions saved to 'prediction-classification-round3.json'\n",
      "Confusion matrix saved as 'confusion_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(labels):\n",
    "    \"\"\"\n",
    "    Calculate inverse class frequencies for weighting\n",
    "    \"\"\"\n",
    "    class_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    class_weights = {\n",
    "        label: total_samples / (len(class_counts) * count)\n",
    "        for label, count in class_counts.items()\n",
    "    }\n",
    "    return class_weights\n",
    "\n",
    "def predict_test_data(model, vectorizer, test_file_path: str, class_weights: dict):\n",
    "    \"\"\"\n",
    "    Predict categories for test data and aggregate by username with balanced voting\n",
    "    \"\"\"\n",
    "    username_predictions = {}\n",
    "    username_texts = {}\n",
    "    \n",
    "    # Read and process test data\n",
    "    with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                username = entry.get('username')\n",
    "                if not username:\n",
    "                    continue\n",
    "                \n",
    "                # Process text\n",
    "                text = process_json_entry(entry)\n",
    "                if text:\n",
    "                    if username not in username_texts:\n",
    "                        username_texts[username] = []\n",
    "                    username_texts[username].append(text)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Make predictions for each username\n",
    "    for username, texts in username_texts.items():\n",
    "        # Transform texts to TF-IDF\n",
    "        text_features = vectorizer.transform(texts)\n",
    "        \n",
    "        # Get predictions for all posts\n",
    "        predictions = model.predict(text_features)\n",
    "        \n",
    "        # Weight the votes based on class distribution\n",
    "        weighted_votes = Counter()\n",
    "        for pred in predictions:\n",
    "            weighted_votes[pred] += class_weights[pred]\n",
    "        \n",
    "        # Get maximum weighted vote\n",
    "        max_weight = max(weighted_votes.values())\n",
    "        \n",
    "        # Get all labels with maximum weighted votes\n",
    "        max_labels = [label for label, weight in weighted_votes.items() \n",
    "                     if weight == max_weight]\n",
    "        \n",
    "        if len(max_labels) == 1:\n",
    "            # Clear winner after weighting\n",
    "            final_prediction = max_labels[0]\n",
    "        else:\n",
    "            # Tie-breaking: use the label with highest confidence scores\n",
    "            # weighted by class distribution\n",
    "            confidences = []\n",
    "            for label in max_labels:\n",
    "                # Get indices where this label was predicted\n",
    "                label_indices = [i for i, pred in enumerate(predictions) if pred == label]\n",
    "                # Get average confidence for this label\n",
    "                label_confidences = model.predict_proba(text_features[label_indices])\n",
    "                avg_confidence = np.mean([conf[list(model.classes_).index(label)] \n",
    "                                       for conf in label_confidences])\n",
    "                # Weight the confidence by class weight\n",
    "                weighted_confidence = avg_confidence * class_weights[label]\n",
    "                confidences.append((label, weighted_confidence))\n",
    "            \n",
    "            # Choose label with highest weighted confidence\n",
    "            final_prediction = max(confidences, key=lambda x: x[1])[0]\n",
    "        \n",
    "        username_predictions[username] = final_prediction\n",
    "    \n",
    "    return username_predictions\n",
    "\n",
    "# Training phase\n",
    "train_file_path = \"processed_dataset-r3v10.jsonl\"\n",
    "test_file_path = \"processed_dataset-r3v10-test.jsonl\"\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "processed_texts, labels = process_jsonl_file(train_file_path)\n",
    "\n",
    "if processed_texts:\n",
    "    # Calculate class weights\n",
    "    print(\"\\nCalculating class weights...\")\n",
    "    class_weights = calculate_class_weights(labels)\n",
    "    print(\"\\nClass distribution weights:\")\n",
    "    for label, weight in class_weights.items():\n",
    "        print(f\"{label}: {weight:.3f}\")\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    print(\"\\nCreating TF-IDF matrix...\")\n",
    "    vectorizer, tfidf_matrix = create_tfidf_matrix(processed_texts)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    results = train_and_evaluate_random_forest(tfidf_matrix, labels)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Training Accuracy: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Training F1 Score: {results['train_f1']:.4f}\")\n",
    "    print(f\"Validation F1 Score: {results['test_f1']:.4f}\")\n",
    "    \n",
    "    # Predict test data with balanced voting\n",
    "    print(\"\\nPredicting test data with balanced voting...\")\n",
    "    predictions = predict_test_data(\n",
    "        results['model'],\n",
    "        vectorizer,\n",
    "        test_file_path,\n",
    "        class_weights\n",
    "    )\n",
    "    \n",
    "    # Save predictions to file\n",
    "    print(\"\\nSaving predictions...\")\n",
    "    with open('prediction-classification-round3.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"\\nPredictions saved to 'prediction-classification-round3.json'\")\n",
    "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processed 10000 lines...\n",
      "Processed 20000 lines...\n",
      "Processed 30000 lines...\n",
      "Processed 40000 lines...\n",
      "Processed 50000 lines...\n",
      "Processed 60000 lines...\n",
      "Processed 70000 lines...\n",
      "Processed 80000 lines...\n",
      "Processed 90000 lines...\n",
      "Finished processing with 0 errors\n",
      "\n",
      "Creating TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yektata/VSCodeProjects/412project/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training balanced Random Forest model...\n",
      "\n",
      "Class distribution:\n",
      "tech: 11801 samples (12.56%)\n",
      "food: 17462 samples (18.59%)\n",
      "health and lifestyle: 17449 samples (18.58%)\n",
      "travel: 9954 samples (10.60%)\n",
      "sports: 3862 samples (4.11%)\n",
      "fashion: 10337 samples (11.01%)\n",
      "entertainment: 10917 samples (11.62%)\n",
      "mom and children: 5175 samples (5.51%)\n",
      "art: 6515 samples (6.94%)\n",
      "gaming: 455 samples (0.48%)\n",
      "\n",
      "Overall Results:\n",
      "Training Accuracy: 0.9914\n",
      "Validation Accuracy: 0.8899\n",
      "Training F1 (micro): 0.9914\n",
      "Training F1 (macro): 0.9889\n",
      "Validation F1 (micro): 0.8899\n",
      "Validation F1 (macro): 0.8857\n",
      "\n",
      "Per-class Performance:\n",
      "\n",
      "Class: art\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: entertainment\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: fashion\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: food\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: gaming\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: health and lifestyle\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: mom and children\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: sports\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: tech\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Class: travel\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-score: 0.0000\n",
      "\n",
      "Predicting test data...\n",
      "\n",
      "Saving predictions...\n",
      "\n",
      "Predictions saved to 'prediction-classification-round3.json'\n",
      "Confusion matrix saved as 'confusion_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_random_forest(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate Random Forest model with class balancing\n",
    "    \"\"\"\n",
    "    # Calculate class weights\n",
    "    class_counts = Counter(y)\n",
    "    total_samples = len(y)\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    # Compute balanced class weights\n",
    "    class_weights = {\n",
    "        label: total_samples / (n_classes * count)\n",
    "        for label, count in class_counts.items()\n",
    "    }\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"{label}: {count} samples ({count/total_samples*100:.2f}%)\")\n",
    "    \n",
    "    # Split the data with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Initialize and train the model with class weights\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200,  # Increased number of trees\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        class_weight=class_weights,  # Add class weights\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        bootstrap=True,  # Enable bootstrapping for better balance\n",
    "        max_samples=0.8  # Use 80% of samples for each tree\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_preds = rf_model.predict(X_train)\n",
    "    test_preds = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    # Calculate F1 scores (both micro and macro for imbalanced cases)\n",
    "    train_f1_micro = f1_score(y_train, train_preds, average='micro')\n",
    "    train_f1_macro = f1_score(y_train, train_preds, average='macro')\n",
    "    test_f1_micro = f1_score(y_test, test_preds, average='micro')\n",
    "    test_f1_macro = f1_score(y_test, test_preds, average='macro')\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, test_preds)\n",
    "    \n",
    "    # Plot confusion matrix with percentages\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues')\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Get unique labels for axis labels\n",
    "    unique_labels = sorted(set(y))\n",
    "    plt.xticks(np.arange(len(unique_labels)) + 0.5, unique_labels, rotation=45)\n",
    "    plt.yticks(np.arange(len(unique_labels)) + 0.5, unique_labels, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    class_metrics = {}\n",
    "    for label in unique_labels:\n",
    "        class_mask_test = (y_test == label)\n",
    "        class_preds_test = (test_preds == label)\n",
    "        \n",
    "        true_pos = np.sum((y_test == label) & (test_preds == label))\n",
    "        total_actual = np.sum(y_test == label)\n",
    "        total_pred = np.sum(test_preds == label)\n",
    "        \n",
    "        precision = true_pos / total_pred if total_pred > 0 else 0\n",
    "        recall = true_pos / total_actual if total_actual > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        class_metrics[label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_f1_micro': train_f1_micro,\n",
    "        'train_f1_macro': train_f1_macro,\n",
    "        'test_f1_micro': test_f1_micro,\n",
    "        'test_f1_macro': test_f1_macro,\n",
    "        'model': rf_model,\n",
    "        'feature_importance': rf_model.feature_importances_,\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': unique_labels,\n",
    "        'class_weights': class_weights,\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "train_file_path = \"processed_dataset-r3v10.jsonl\"\n",
    "test_file_path = \"processed_dataset-r3v10-test.jsonl\"\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "processed_texts, labels = process_jsonl_file(train_file_path)\n",
    "\n",
    "if processed_texts:\n",
    "    # Create TF-IDF matrix\n",
    "    print(\"\\nCreating TF-IDF matrix...\")\n",
    "    vectorizer, tfidf_matrix = create_tfidf_matrix(processed_texts)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    print(\"\\nTraining balanced Random Forest model...\")\n",
    "    results = train_and_evaluate_random_forest(tfidf_matrix, labels)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nOverall Results:\")\n",
    "    print(f\"Training Accuracy: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Training F1 (micro): {results['train_f1_micro']:.4f}\")\n",
    "    print(f\"Training F1 (macro): {results['train_f1_macro']:.4f}\")\n",
    "    print(f\"Validation F1 (micro): {results['test_f1_micro']:.4f}\")\n",
    "    print(f\"Validation F1 (macro): {results['test_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class Performance:\")\n",
    "    for label, metrics in results['class_metrics'].items():\n",
    "        print(f\"\\nClass: {label}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-score: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Predict test data with balanced voting\n",
    "    print(\"\\nPredicting test data...\")\n",
    "    predictions = predict_test_data(\n",
    "        results['model'],\n",
    "        vectorizer,\n",
    "        test_file_path,\n",
    "        results['class_weights']\n",
    "    )\n",
    "    \n",
    "    # Save predictions\n",
    "    print(\"\\nSaving predictions...\")\n",
    "    with open('prediction-classification-round3.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"\\nPredictions saved to 'prediction-classification-round3.json'\")\n",
    "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
