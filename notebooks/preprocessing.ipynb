{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Keys to check in the \"profile\" subtree\n",
    "keys_to_check = [\n",
    "    \"is_guardian_of_viewer\",\n",
    "    \"is_supervised_by_viewer\",\n",
    "    \"is_supervised_user\",\n",
    "    \"is_supervision_enabled\",\n",
    "    \"overall_category_name\",\n",
    "    #\"is_business_account\",\n",
    "    \"is_private\",\n",
    "    #\"is_professional_account\",\n",
    "    #\"is_verified\",\n",
    "    \"is_verified_mv4b\"\n",
    "]\n",
    "\n",
    "# Function to check if a value is not false or null\n",
    "def has_unexpected_value(value):\n",
    "    return value not in (False, None, \"\")  # Also treating empty strings as invalid\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = \"training-dataset.jsonl\"\n",
    "\n",
    "# Open and process the JSONL file\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "        try:\n",
    "            # Parse the JSON object from the line\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Access the \"profile\" subtree if it exists\n",
    "            profile = data.get(\"profile\", {})\n",
    "            \n",
    "            # Check each key in the \"profile\" subtree\n",
    "            for key in keys_to_check:\n",
    "                if key in profile and has_unexpected_value(profile[key]):\n",
    "                    print(f\"Line {line_number}: {key} in 'profile' has an unexpected value: {profile[key]}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_number}: Invalid JSON - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to input and output files\n",
    "input_file = \"training-dataset.jsonl\"\n",
    "output_file = \"processed_dataset-r3v1.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "# Keys to retain in the \"posts\" and \"profile\" sections\n",
    "posts_keys = [\"caption\", \"id\", \"comments_count\", \"like_count\", \"timestamp\"]\n",
    "profile_keys = [\n",
    "    \"username\",\n",
    "    \"biography\",\n",
    "    \"entities\",\n",
    "    \"follower_count\",\n",
    "    \"following_count\",\n",
    "    \"hide_like_and_view_counts\",\n",
    "    \"highlight_reel_count\",\n",
    "    \"is_business_account\",\n",
    "    \"is_professional_account\",\n",
    "    \"is_verified\",\n",
    "    \"post_count\"\n",
    "]\n",
    "merge_keys = [\"business_category_name\", \"category_enum\", \"category_name\"]\n",
    "\n",
    "# Convert numeric value to logarithmic scale, handling edge cases\n",
    "def to_log_scale(value):\n",
    "    try:\n",
    "        value = float(value)\n",
    "        return math.log10(value) if value > 0 else 0\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = \"training-dataset.jsonl\"\n",
    "output_file = \"processed_dataset-r3v1.jsonl\"\n",
    "\n",
    "# Process the JSONL file\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        try:\n",
    "            # Parse the JSON object from the line\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Process \"posts\" section\n",
    "            posts = data.get(\"posts\", {})\n",
    "            filtered_posts = {key: posts[key] for key in posts_keys if key in posts}\n",
    "            \n",
    "            # Apply log transformation to \"comments_count\" and \"like_count\"\n",
    "            for key in [\"comments_count\", \"like_count\"]:\n",
    "                if key in filtered_posts:\n",
    "                    filtered_posts[key] = to_log_scale(filtered_posts[key])\n",
    "            \n",
    "            # Process \"profile\" section\n",
    "            profile = data.get(\"profile\", {})\n",
    "            filtered_profile = {key: profile[key] for key in profile_keys if key in profile}\n",
    "            \n",
    "            # Merge categories\n",
    "            filtered_profile[\"merged_category\"] = next(\n",
    "                (profile[key] for key in merge_keys if profile.get(key)), None\n",
    "            )\n",
    "            \n",
    "            # Apply log transformation to \"follower_count\" and \"following_count\"\n",
    "            for key in [\"follower_count\", \"following_count\"]:\n",
    "                if key in filtered_profile:\n",
    "                    filtered_profile[key] = to_log_scale(filtered_profile[key])\n",
    "            \n",
    "            # Combine the processed data\n",
    "            processed_data = {\n",
    "                \"posts\": filtered_posts,\n",
    "                \"profile\": filtered_profile\n",
    "            }\n",
    "            \n",
    "            # Write the processed JSON object back to the file\n",
    "            outfile.write(json.dumps(processed_data) + \"\\n\")\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_number}: Invalid JSON - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Process \"posts\" section\u001b[39;00m\n\u001b[1;32m     40\u001b[0m posts \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposts\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m     41\u001b[0m filtered_posts \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 42\u001b[0m     key: to_log_scale(posts[key]) \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomments_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlike_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mposts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m posts_keys\n\u001b[1;32m     44\u001b[0m }\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Process \"profile\" section\u001b[39;00m\n\u001b[1;32m     47\u001b[0m profile \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofile\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "# Keys to retain in the \"posts\" and \"profile\" sections\n",
    "posts_keys = [\"caption\", \"id\", \"comments_count\", \"like_count\", \"timestamp\"]\n",
    "profile_keys = [\n",
    "    \"biography\",\n",
    "    \"entities\",\n",
    "    \"follower_count\",\n",
    "    \"following_count\",\n",
    "    \"hide_like_and_view_counts\",\n",
    "    \"highlight_reel_count\",\n",
    "    \"is_business_account\",\n",
    "    \"is_professional_account\",\n",
    "    \"is_verified\",\n",
    "    \"post_count\"\n",
    "]\n",
    "merge_keys = [\"business_category_name\", \"category_enum\", \"category_name\"]\n",
    "\n",
    "# Convert numeric value to logarithmic scale, handling edge cases\n",
    "def to_log_scale(value):\n",
    "    try:\n",
    "        value = float(value)\n",
    "        return math.log10(value) if value > 0 else 0\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = \"training-dataset.jsonl\"\n",
    "output_file = \"processed_dataset-r3v2.jsonl\"\n",
    "\n",
    "# Process the JSONL file\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        try:\n",
    "            # Parse the JSON object from the line\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Process \"posts\" section\n",
    "            posts = data.get(\"posts\", {})\n",
    "            filtered_posts = {\n",
    "                key: to_log_scale(posts[key]) if key in [\"comments_count\", \"like_count\"] else posts.get(key, \"\")\n",
    "                for key in posts_keys\n",
    "            }\n",
    "            \n",
    "            # Process \"profile\" section\n",
    "            profile = data.get(\"profile\", {})\n",
    "            filtered_profile = {key: profile.get(key, \"\") for key in profile_keys}\n",
    "            \n",
    "            # Merge categories\n",
    "            filtered_profile[\"merged_category\"] = next(\n",
    "                (profile.get(key, \"\") for key in merge_keys if profile.get(key)), None\n",
    "            )\n",
    "            \n",
    "            # Apply log transformation to \"follower_count\" and \"following_count\"\n",
    "            for key in [\"follower_count\", \"following_count\"]:\n",
    "                if key in filtered_profile:\n",
    "                    filtered_profile[key] = to_log_scale(filtered_profile[key])\n",
    "            \n",
    "            # Combine the processed data\n",
    "            processed_data = {\n",
    "                \"posts\": filtered_posts,\n",
    "                \"profile\": filtered_profile\n",
    "            }\n",
    "            \n",
    "            # Write the processed JSON object back to the file\n",
    "            outfile.write(json.dumps(processed_data) + \"\\n\")\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_number}: Invalid JSON - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 2191: Error - 'like_count'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "# Keys to retain in the \"posts\" and \"profile\" sections\n",
    "posts_keys = [\"caption\", \"id\", \"comments_count\", \"like_count\", \"timestamp\"]\n",
    "profile_keys = [\n",
    "    \"username\",\n",
    "    \"biography\",\n",
    "    \"entities\",\n",
    "    \"follower_count\",\n",
    "    \"following_count\",\n",
    "    \"hide_like_and_view_counts\",\n",
    "    \"highlight_reel_count\",\n",
    "    \"is_business_account\",\n",
    "    \"is_professional_account\",\n",
    "    \"is_verified\",\n",
    "    \"post_count\"\n",
    "]\n",
    "merge_keys = [\"business_category_name\", \"category_enum\", \"category_name\"]\n",
    "\n",
    "# Convert numeric value to logarithmic scale, handling edge cases\n",
    "def to_log_scale(value):\n",
    "    try:\n",
    "        value = float(value)\n",
    "        return math.log10(1+value) if value > 0 else 0\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = \"training-dataset.jsonl\"\n",
    "output_file = \"processed_dataset-r3v4.jsonl\"\n",
    "\n",
    "# Process the JSONL file\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        try:\n",
    "            # Parse the JSON object from the line\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Process \"posts\" section\n",
    "            posts = data.get(\"posts\", [])\n",
    "            if isinstance(posts, list):\n",
    "                filtered_posts = [\n",
    "                    {\n",
    "                        key: to_log_scale(post[key]) if key in [\"comments_count\", \"like_count\"] else post.get(key, \"\")\n",
    "                        for key in posts_keys\n",
    "                    }\n",
    "                    for post in posts\n",
    "                ]\n",
    "            else:\n",
    "                filtered_posts = {\n",
    "                    key: to_log_scale(posts[key]) if key in [\"comments_count\", \"like_count\"] else posts.get(key, \"\")\n",
    "                    for key in posts_keys\n",
    "                }\n",
    "            \n",
    "            # Process \"profile\" section\n",
    "            profile = data.get(\"profile\", {})\n",
    "            filtered_profile = {key: profile.get(key, \"\") for key in profile_keys}\n",
    "            \n",
    "            # Merge categories\n",
    "            filtered_profile[\"merged_category\"] = next(\n",
    "                (profile.get(key, \"\") for key in merge_keys if profile.get(key)), None\n",
    "            )\n",
    "            \n",
    "            # Apply log transformation to \"follower_count\" and \"following_count\"\n",
    "            for key in [\"follower_count\", \"following_count\"]:\n",
    "                if key in filtered_profile:\n",
    "                    filtered_profile[key] = to_log_scale(filtered_profile[key])\n",
    "            \n",
    "            # Combine the processed data\n",
    "            processed_data = {\n",
    "                \"posts\": filtered_posts,\n",
    "                \"profile\": filtered_profile\n",
    "            }\n",
    "            \n",
    "            # Write the processed JSON object back to the file\n",
    "            outfile.write(json.dumps(processed_data) + \"\\n\")\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_number}: Invalid JSON - {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Line {line_number}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text with Unicode escapes\n",
    "sample_text = \"U19 Mill\\u00ee Tak\\u0131m\\u0131m\\u0131z Antalya'da oynayaca\\u011f\\u0131 UEFA Avrupa U19 \\u015eampiyonas\\u0131 eleme turu ma\\u00e7lar\\u0131 haz\\u0131rl\\u0131klar\\u0131n\\u0131 8 Kas\\u0131m \\u00c7ar\\u015famba g\\u00fcn\\u00fcnden bu yana Arslan Zeki Demirci Spor Kompleksi'nde yapt\\u0131\\u011f\\u0131 antrenmanlarla s\\u00fcrd\\u00fcr\\u00fcyor.\"\n",
    "\n",
    "# Decode Unicode escape sequences\n",
    "def decode_unicode(text):\n",
    "    return text.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "# Decode the sample text\n",
    "decoded_text = decode_unicode(sample_text)\n",
    "\n",
    "# Additional preprocessing for Turkish text (optional)\n",
    "def preprocess_text(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation (if needed)\n",
    "    text = \"\".join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text\n",
    "\n",
    "# Preprocess the decoded text\n",
    "preprocessed_text = preprocess_text(decoded_text)\n",
    "\n",
    "# Print decoded and preprocessed text\n",
    "print(\"Decoded Text:\")\n",
    "print(decoded_text)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n",
    "\n",
    "# Example TF-IDF vectorization with the preprocessed text\n",
    "texts = [preprocessed_text]  # You can add more Turkish texts here\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Print TF-IDF matrix and feature names\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "print(\"\\nFeature Names:\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the mapping for Turkish characters to their letter counterparts\n",
    "turkish_to_latin = {\n",
    "    '\\u0131': 'i', '\\u0130': 'I',\n",
    "    '\\u00e7': 'c', '\\u00c7': 'C',\n",
    "    '\\u011f': 'g', '\\u011e': 'G',\n",
    "    '\\u00f6': 'o', '\\u00d6': 'O',\n",
    "    '\\u015f': 's', '\\u015e': 'S',\n",
    "    '\\u00fc': 'u', '\\u00dc': 'U'\n",
    "}\n",
    "\n",
    "def replace_turkish_characters(text):\n",
    "    for turkish, latin in turkish_to_latin.items():\n",
    "        text = text.replace(turkish, latin)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = \"processed_dataset-r3v4.jsonl\"\n",
    "output_file = \"processed_dataset-r3v5.jsonl\"\n",
    "\n",
    "\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line_number, line in enumerate(infile, start=1):\n",
    "        try:\n",
    "            # Parse the JSON object from the line\n",
    "            data = json.loads(line)\n",
    "            # Replace Turkish characters in the captions and biography\n",
    "            for post in data['posts']:\n",
    "                if post['caption'] != None:\n",
    "                    post['caption'] = replace_turkish_characters(post['caption'])\n",
    "\n",
    "            if data['profile']['biography'] != None:\n",
    "                data['profile']['biography'] = replace_turkish_characters(data['profile']['biography'])\n",
    "\n",
    "            if data['profile']['entities'] != None:\n",
    "                data['profile']['entities'] = replace_turkish_characters(data['profile']['entities'])\n",
    "\n",
    "            # Convert back to JSONL string\n",
    "            #modified_jsonl_data = json.dumps(data, ensure_ascii=False, indent=4)\n",
    "\n",
    "            # Print the modified JSONL data\n",
    "            outfile.write(json.dumps(data) + \"\\n\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line {line_number}: Invalid JSON - {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Line {line_number}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been processed and saved to processed_dataset-r3v7.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import emoji\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"processed_dataset-r3v5.jsonl\"\n",
    "output_file =  \"processed_dataset-r3v7.jsonl\"\n",
    "\n",
    "\n",
    "def extract_emojis_and_tags(text):\n",
    "    \"\"\"Separate emojis and hashtags from text.\"\"\"\n",
    "    if not text:  # Handle None or empty string\n",
    "        return \"\", \"\", []\n",
    "    emojis = ''.join(char for char in text if char in emoji.EMOJI_DATA)\n",
    "    hashtags = re.findall(r\"#\\w+\", text)\n",
    "    clean_text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n",
    "    clean_text = ''.join(char for char in clean_text if char not in emoji.EMOJI_DATA)  # Remove emojis\n",
    "    return clean_text.strip(), emojis, hashtags\n",
    "\n",
    "# Open the input file and process line by line\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        # Parse the JSON object\n",
    "        record = json.loads(line)\n",
    "\n",
    "        # Process posts\n",
    "        if \"posts\" in record:\n",
    "\n",
    "            for post in record[\"posts\"]:\n",
    "                # Handle missing or null numeric fields\n",
    "                post[\"comments_count\"] = post.get(\"comments_count\", 0) or 0\n",
    "                post[\"like_count\"] = post.get(\"like_count\", 0) or 0\n",
    "\n",
    "                # Convert timestamp to year, month, day, hour\n",
    "                if \"timestamp\" in post and post[\"timestamp\"]:\n",
    "                    ts = post[\"timestamp\"]\n",
    "                    try:\n",
    "                        dt = datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S\")\n",
    "                        post[\"year\"] = dt.year\n",
    "                        post[\"month\"] = dt.month\n",
    "                        post[\"day\"] = dt.day\n",
    "                        post[\"hour\"] = dt.hour\n",
    "                    except ValueError:\n",
    "                        post[\"year\"], post[\"month\"], post[\"day\"], post[\"hour\"] = None, None, None, None\n",
    "                    del post[\"timestamp\"]\n",
    "\n",
    "                # Separate emojis, hashtags, and clean text in captions\n",
    "                if \"caption\" in post and post[\"caption\"]:\n",
    "                    clean_caption, emojis, hashtags = extract_emojis_and_tags(post[\"caption\"])\n",
    "                    post[\"clean_caption\"] = clean_caption\n",
    "                    post[\"emojis\"] = emojis\n",
    "                    post[\"hashtags\"] = hashtags\n",
    "                    del post[\"caption\"]\n",
    "                else:\n",
    "                    post[\"clean_caption\"] = \"\"\n",
    "                    post[\"emojis\"] = \"\"\n",
    "                    post[\"hashtags\"] = []\n",
    "\n",
    "            # Calculate engagement rate\n",
    "            follower_count = record[\"profile\"].get(\"follower_count\", 0) or 0\n",
    "\n",
    "            \n",
    "\n",
    "        # Handle missing/null values in the profile section\n",
    "        for key, value in record[\"profile\"].items():\n",
    "            if value is None:\n",
    "                if isinstance(value, str):\n",
    "                    record[\"profile\"][key] = \"\"\n",
    "                elif isinstance(value, (int, float)):\n",
    "                    record[\"profile\"][key] = 0\n",
    "\n",
    "        record[\"profile\"][\"biography\"],record[\"profile\"][\"bio-emojis\"],record[\"profile\"][\"bio-hashtags\"] = extract_emojis_and_tags(record[\"profile\"][\"biography\"])\n",
    "\n",
    "        \n",
    "        record[\"profile\"][\"entities\"],record[\"profile\"][\"ent-emojis\"],record[\"profile\"][\"ent-hashtags\"] = extract_emojis_and_tags(record[\"profile\"][\"entities\"])\n",
    "\n",
    "        # Write the modified record to the output file\n",
    "        outfile.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Data has been processed and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Step 1: Read the train-classification.csv file and create a mapping of usernames to categories\n",
    "category_mapping = {}\n",
    "with open('train-classification.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)  # Skip the header\n",
    "    for row in csv_reader:\n",
    "        username, category = row\n",
    "        category_mapping[username] = category\n",
    "\n",
    "# Step 2: Read the .jsonl file and process each line\n",
    "output_lines = []\n",
    "with open('processed_dataset-r3v7.jsonl', 'r') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        data = json.loads(line)\n",
    "        username = data['profile']['username']\n",
    "\n",
    "        # Step 3: Check if the username exists in the mapping\n",
    "        if username in category_mapping:\n",
    "            # Step 4: Add the category to the profile section\n",
    "            category_to_add = category_mapping[username]\n",
    "            data['profile']['train_category'] = category_to_add.lower()\n",
    "            output_lines.append(json.dumps(data))\n",
    "\n",
    "# Step 5: Write the modified lines to a new .jsonl file\n",
    "with open('processed_dataset-r3v8-v2.jsonl', 'w') as output_file:\n",
    "    for output_line in output_lines:\n",
    "        output_file.write(output_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Read the .jsonl file and process each line\n",
    "output_lines = []\n",
    "with open('processed_dataset-r3v8-v2.jsonl', 'r') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        data = json.loads(line)\n",
    "        profile_data = data['profile']\n",
    "\n",
    "        # Step 2: For each post, merge the profile data\n",
    "        for post in data['posts']:\n",
    "            post.update(profile_data)\n",
    "            output_lines.append(json.dumps(post))\n",
    "\n",
    "# Step 3: Write the flattened posts to a new .jsonl file\n",
    "with open('processed_dataset-r3v10.jsonl', 'w') as output_file:\n",
    "    for output_line in output_lines:\n",
    "        output_file.write(output_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entertainment\n",
      "Fashion\n",
      "Gaming\n",
      "Mom and Children\n",
      "Art\n",
      "Sports\n",
      "Health and Lifestyle\n",
      "Tech\n",
      "Travel\n",
      "Food\n",
      "Health and lifestyle\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Initialize a set to store unique train_categories\n",
    "unique_train_categories = set()\n",
    "\n",
    "# Open the JSONL file and read it line by line\n",
    "with open('processed_dataset-r3v9.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse the JSON object from the line\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Extract the train_category\n",
    "        train_category = data.get('train_category')\n",
    "\n",
    "        # Add the train_category to the set of unique categories\n",
    "        if train_category:\n",
    "            unique_train_categories.add(train_category)\n",
    "\n",
    "# Print the unique train_categories\n",
    "for category in unique_train_categories:\n",
    "    print(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['livapastanesi', 'barisgross', 'tusasshop', 'etolyadigital', 'tugrulonur', 'tulugozlu', 'gokidy', 'cengizgumus_official', 'krossbisiklet', 'haribochamallows', 'ozatashipyard', 'yenisafak', 'iamsiddeshjadhav', 'burcinterzioglu', 'steakhousegunaydin', 'baselifeclub', 'benismailyildirimm', 'imuneksfarma', 'dogakoyucatalca', 'sena.sener', 'kandilliborsarestaurant', 'selamiersoyy', 'deutz_fahr_turkey', 'cevaheer', 'tezatsanat', 'filtresizcom', 'palomamarina_suites', 'westchocolatemarina', 'sebnemcapa', 'rozetsepeti', 'ececesmioglu', 'ustapidecitr', 'gocaagonyali', 'maestro.sanat.kursu', 'oztayteksofficial', 'imtolstoyevski', 'turkervip', 'rustik.rus.restorani', 'guzellik_hemsiresi', 'mehmetcikvakfi', 'bruderturkiye', 'brandingturkiye', 'dogushyilmaz', 'crmbelediyesi', 'gcaturkiye', 'bskcemildeveci', 'lokmanhekimhastaneleri', 'trumpavm', 'resimlimagnet_com', 'ilaydaakdogan', 'karincaproduksiyon', 'ilkergodencemobilya', 'semihasahin', 'ogretmeninkaleminden_', 'narkoyjournal', 'kuzucu_mucevherat', 'mmkmetalurji', 'okanyanmazcom', 'silopibld', 'cubocomtr', 'ahlatbld', 'tarimoz', 'komunpub', 'novax_gozluk_camlari', 'tarkan', 'sefkatpeker', 'kore_kultur_merkezi', 'gaiaandco', 'bvbajans', 'esila_kids', 'halisunasyoncoffee', 'ecovero.turkey', 'denizliotizmdernegi', 'buckheadturkey', 'karaman.bel.tr', 'imoykukarayel', 'celsuskitabevi', 'sonerarica1', 'keyifbebesi', 'vadistanbul', 'pivoraizmir', 'aliyagciii', 'bebebiyat', 'erdqtr', 'mo_istanbul', 'mekpan_panel', 'rotarttasarim', 'tc_giresunvaliligi', 'yigitcaliskan_', 'craftdesignistanbul', 'jciistanbul', 'mustafakoleoglu', 'ankarabbld', 'staynewinn', 'hasvetmedikal', 'trendysidebeach', 'promosyonizmir', 'turkiyekickboksfederasyonu', 'tugcedural', 'nestleturkiye', 'bomotorsport', 'ahmetmaranki', 'hayalevent', 'ozelkonakhastanesi', 'istanbul_halidunyasi', 'adiyaman_park_avm', 'pillavidenizli', 'annebebekdergisii', 'enmutluoldugumyer', 'alfaglb', 'erciyes.anadolu', 'dytgozdembasarir', 'sirenebelekhotel', 'ahbap', 'uzmanturizm', 'cagdasonurozturk', 'globaldentaleskisehir', 'kilicyarenn', 'moisahne', 'thecitysuitestr', 'kayihanpala', 'bilkenttiyatro', 'kentmeydaniavm', 'cnridentex', 'kirencekaya_balikcisi', 'getir', 'etimesgutbld', 'yigitpoyrazz', 'gokcinarnecdet', 'abdullahozdemirbb', 'babilkulesi_', 'remedassistance', 'mikronajans', 'huaweimobiletr', 'gezmedenbilemezsin', 'coskunaltun36', 'lebleukusadasi', 'schweppes_tr', 'pilatesannesi', 'livingroomist', 'transeshairtransplant', 'duygubie', 'mylittleponytr', 'gupseo', 'erenoren', 'kirmiziocakbasi', 'duzcehalkmar', 'unicosigorta', 'gezginsincap', 'erkek_bileklik_modelleri', 'yapikredibomontiada', 'socratesbistro', 'mutfaktayusufvar', 'digitalagencynetwork', 'fiatkastamonugozde', 'brkhkmn', 'vaziyetcomtr', 'lisebjkcom', 'sinanguler', 'mamaspapas.tr', 'generalmobile', 'borusanlojistik', 'sana_turkiye', 'viptrend.com.tr', 'artonistanbul', 'sezergsm38', 'resoundturkiye', 'tiyatroyeniden_official', 'barcintancan', 'kadindayanismavakfi', 'serdarsandal35', 'denizcanaktasofficial', 'gulben123', 'gezmelerdeyim', 'gomecbelediyesi', 'serpilexclusive', 'gsb_hatay', 'talyakizyurdu', 'aycasenbaskan', 'mandolinyayinlari', 'sadberkhanimmuzesi', 'zamanebaba', 'izzettinyilmaztr', 'freshscarfs', 'pasta34shop', 'webajans', 'gci.tuerkei', 'esseklinik', 'samuthatr', 'ozelaydingoz', 'iloveanalogue', 'kadinlarfestivali', 'braunhouseholdturkey', 'este.foot', 'uzmananahtar', 'yorglass', 'selcuklukongremerkezi', 'zerrintekindor', 'nihatcan11', 'sarikizmadensuyu', 'ekremcoskundoner', 'ilhansen', 'hacettepe_university', 'besiktasbelediyesi', 'scaniatr', 'tosunpasaalacati', 'tuna.food', 'indirimlix', 'theurbangoat.ist', 'kesfetmek_guzeldir', 'beko_serdar', 'mgulluoglu', 'muyashop', 'serhanonat', 'alperenduymaz', 'tasarimgroup', 'ege_koy_mahsulleri', 'yamalioglumucevherat', 'babylissturkiye', 'perlavista_avm', 'dr.gokalpdizdar', 'kadinkoalisyonu', 'acemdenizyel', 'turkerkilic', 'gulsenbubikogluofficial', 'yazyuceil', 'kapadokyazeppelin', 'ikbalkaya', 'lastdoor_tr', 'dengemerkezi', 'edisdeposu', 'akyuzmobilyaurfa', 'solcugazete', 'avmorion', 'fizikseltiyatro', 'gurmerehber', 'gursubeltr', 'sahanepazar', 'swipeline', 'ahmethamdigork', 'expertyol', 'araskarafil', 'tepebasibeltr', 'kumpirbox', 'hedefbeytotel', 'seyitaliofficial', 'armonipark', 'erzincansuleymankaraman', 'cebirecep', 'omsvibro', 'alacatibeachresort', 'elkon_official', 'kaan.sekban', 'flotalmirror', 'thelevanttahinier', 'zodyakli', 'beiniztv', 'mesutscl', 'umraniyebeltr', 'muratacilimranli', 'imamoglu.belediyesi', 'jumbokunefetr', 'gulgrupgayrimenkul', 'soyturkrabia', 'pekcetintemizlik', 'psikezone', 'donerci_serkan_usta', 'mirdellav', 'iyigelir.biz', 'durumle', '3durak', 'roofgamesstudio', 'konseptkutucom', 'uras.benlioglu', 'yemekcom', 'ipekozagan', 'ozeltanyeri', 'hakkiakdenizz', 'mekoglobal', 'datcabelediyesi', 'tkdkordu', 'yelkenrest', 'suleymansarilar', '___daire___', 'iremhlvcioglu', 'uberkuloz', 'fdnsirketlergrubu', 'podyumsanatmahal', '3eeemedya', 'akinyucel', 'sky_uk', 'kesifmekan', 'whatupmell', 'nedimkaplann', 'ozdilekusakavm', 'trendmobilyacom', 'beka_beka_anaokulu', 'turkiyedagcilik', 'leylasapmazz', 'turkistanpazari', 'hyattregencyistanbulatakoy', 'gercekmutfak', 'tolgasaritas', 'gayretmakina', 'gzonemag', 'lineersoft', 'desimerkez', 'tavukdunyasikariyer', 'akgunwalker', 'didemeryarunlu', 'asaspen.veratec', 'ezgi_basaran', 'applegurgencler', 'turktraktorkariyer', 'nihatatli.tr', 'guncesanattiyatrosu', 'talha_karci', 'meltem.hastanesi', 'diyetisyengamzealtinay', 'canyucetas', 'kaanurgancioglu', 'gursueraykirtasiye', 'dinersclubtr', 'stonebarizmir', 'rocscoffee', 'ayhancanguven', 'ezomola', 'unuttumsanmacom', 'hotech_ecosystem', 'bingolbel', 'cengizsemercio', 'bskulgurgokhan_', 'veritameyve', 'bekiraltantr', 'vmilor', 'pylsmn', 'duzceguvencomtr', 'argonotlar.sanat', 'cilemakar', 'novotel_bosphorus', 'yuncualimagazalari', 'bahar_eli', 'turkiyesualtisporlari', 'operaistanbul', 'rengarenkkanal', 'guronimobilya', 'ktheventagency', 'lutfusavas', 'cookluhayat', 'birayetarapca', 'buyukdere35', 'milliyiyici', 'muhikucom', 'izmirekolhastanesi', 'fundafashion', 'dilart.design', 'eurovisn_turkey', 'ibrahim__varli', 'tarsusbelediyesi', 'pinarhepyanimda', 'sporsuncom', 'btginsaat', 'izmiretkinlikhaberleri', 'akifergurumtr', 'boztaspremium', '3d_maket', 'tekinozalit', 'elvanreklamtr', 'alikerimdiler', 'tufanlarkuyumculuk', 'sendeyapsana', 'arifibrahimoff', 'alpatayazilim', 'kasaphansteakhouse', 'seturdutyfree', 'ozkocengin', 'novikovbodrum', 'yedibilgeler', 'caddekultursanat', 'weareomgteam', 'bullentoz', 'huseyinkabisteke', 'takviyegiller', 'herseyyanimda', '_helinkandemir', 'ipekpamuk', 'gsb_erzincan', 'seyirdernegi', 'eformspormerkezi', 'hakansuer41', 'medicasimple', 'murat.besikci', 'zadevital', 'gokhanozoguz', 'fishorg', 'ahlparkavm', 'galipensarioglu', 'acarflowers', 'selcuklubel', 'onlembebekbezi', 'liqui_moly_turkey', 'magnumenerji', 'kulabelediyesi', 'aleynaclsknn', 'ikicay', 'lemonadeagency', 'unolezzetleri', 'pelinyarr', 'viastudio', 'hyattregencyizmir_istinyepark', 'mentos_tr', 'meliskaracam', 'dedemankonyahotel', '01burdaavm', 'minimokirtasiye', 'pendikbelediyesi', 'arkas_spor', 'normholding', 'ahmetipeksucuklariafyon', 'medyaevi_iletisim', 'mbbkulturas', 'byelergumus', 'ozanyigitt', 'miniaturkmuzesi', 'ozgeyagizz', 'nanopax_com', 'roketsan', 'kadikoyemek', 'ozaysendir', 'ferideozdincc', 'ecift', 'eklerciumit', 'estehair', 'izmirhim', 'demiirhaan', 'ketebe', 'pozitif.dusunceler', 'opdrerdemzengin', 'gufojewelry', 'hamzaefendi', 'imok_studio', 'ysdistanbul', 'beachandbeyondswimwear', 'richmondpamukkale', 'rizepazarbelediyesi', 'vineturkiyem', 'ikizlerbebefethiye', 'trtworld', 'ozgefiskin', 'engeenergy', 'biostoretr', 'edakok59', 'avmpomelon', 'sait.restaurant', 'evimin.yemekleri', 'ladymiena', 'dokusanatatolyesi', 'alvisajanss', 'birunitiyatro', 'ugursengulx', 'kemerlizimba', 'drenginocal', 'patara.well', 'barbarosfarm', 'ataguroffice', 'ankaranobeltipkitabevleri', 'arminesiirtpark', 'bancocomtr', 'akbarkodetiket', 'karsiyakabelediyesi', 'simaybarlass', 'erdogantok55', 'levonbagis', 'quartzclinique', 'porlandgastro', 'dyt.psk.bilgesakli', 'dericompany', 'keciorenbelediyesi06', 'nergiscorakci', 'ontangrup', 'katalportmarin', 'primemallantakya', 'umit_ozlale', 'lokmacafe', 'drhalilibrahimtekin', 'deppo_avantaj', 'meltem.kursunlu', 'akhisarpresshaber', 'grisahne', 'besaygold', 'bungalovoteller', 'mersinsensin', 'biancaboya', 'yegezokuyaz', 'cemseymen', 'prohealthgenetictests', 'erenkoy_danismanlik', 'ayvalikescbilisim', 'thecompanytr', 'iyigelecekelcileri', 'nevzatozkur0', 'denizlibbld', 'cibalikapibalikcisi', 'pak_metal', 'tavtechnologies', 'malatyapark', 'onurkuyumcu1919', 'poyrazotomotiv', 'ninovaparkavm', 'drderyacan', 'primepuritypep', 'galenikecza', 'milangaz', 'mediviasaglikgrubu', 'tulayerciyaskayaa', 'kurulusdizisi', 'nurgulyesilcayy', 'selinayalcn', 'calgontr', 'mplusturkiye', 'antmarealacati', 'duzicibld', 'assosdiamond', 'gulseli.com.tr', 'iremsak', 'zulaoyun', 'ozdilekboluavm', 'everfastenings', 'ozakgokturk', 'pelinkarahan', 'kerimsabanci', 'fatmasamsayilmaz', 'drbrkzks', 'ciftelerbelediyesi', 'serangocer', 'hisliseyler', 'richhousebistro', 'nurullahsavasresmi', 'yfatihcoban', 'boschturkiyekariyer', 'yachtmurat', 'cetinkayapano', 'yoldabiblog', 'limak.hotels', 'eminoguzcelebi', 'sailturkiyeracing', 'kronospan', 'defneilgaz.com.tr', 'seyfidingil', 'atalaraleyna', '253binyasin', 'tulinnalbayrak', 'reiskuyumculuk', 'dijiatolye', 'antolojiankara', 'girafestore', 'profdrmuratbas', 'kendinemuzisyen', 'didembalikofficial', 'zpor', 'aquayasamofficial', 'oguzhankapilar', 'westparkoutlet', 'odyomedbursa', 'pluscomiletisim', 'ulubeybelediyesi', 'galvinnisantasi', 'yoldalogistics', 'mesdokum', 'cizmecikedi', 'koleksiyonerberber', 'gurhanaltundasar', 'ankaragucufp', 'technical_sharing', 'idaorgonite', 'virahabercom', 'uykudunyasicom', 'beymenclub', 'theboviera', 'logoyazilimkariyer', 'tijenaktay', 'mastercardturkiye', 'itef_istanbul', 'buyukekerbijon', 'kosbsocial', 'bisalt', 'antmodern', 'trendyolhizlimarket', 'transparentproductions', 'acibadem.international', 'mervekutlu', 'dirim.metal', 'rahmi.gencer', 'hamzayilmazmucevherat', '1001sanat', 'gaziantepbeld', 'aslanegegrup', 'sabanozubelediyesi', 'flotalofficial', 'guralpremier', 'debuttiyatro', 'yorkkadikoy', 'spitalispecialbahceci', 'ecebelenn', 'ersinceliq', 'greysatelier', 'sarigolbelediyesi', 'umut.cigercisi', 'ikuakingucoditoryumu', 'tcvanvaliligi', 'sivilsayfalar', 'sestri.lifestyle', 'crafttiyatro', '1983beyoglu', 'ardahalil', 'sinemunsal', 'baziseylertiyatrosu', 'ipekkyazici', 'junkeralsancak', 'bihterkocc', 'alinuraktas', 'forkidstyle', 'mesudiyelicom', 'hurriyetkelebek', 'turktelekombasketbol', 'drselcukyuce', 'andacyesilyurt', 'yigitcanic', 'gulercetingoz', 'paktplus', 'tiyatrokast', 'akutdernegi', 'telesuresigorta', 'aperinastudios', 'yenicekoyu43', 'gencliksporbak', 'gittibu_com', 'farukerdem', 'zamanekahvesi', 'ismailsenol', 'senpilic', 'alternatifmarket', 'sehernigiz', 'ozulkutipmerkezi', 'acunilicali', 'adnanturankb', 'ckalebelediye', 'nesrullah.tanglay', 'aslieliftanugursamanci', 'tonyhawk', 'pursuailesi', 'call_me_mumsy', 'kocamaar', 'retinagraphic', 'winblocktr', 'multiplayertv', 'beyoglubld', 'm0rtyrick', 'icindekocvar', 'gshocktr', 'beykozkultursanat', 'ecmelsoylu', 'tevadimadim', 'bursayasam', 'maden_belediyesi', 'trgyguler', 'metanucreative', 'upl.turkey', 'celebizadeonline', 'uclermarket', 'toyganavanoglu', 'agacin.izinde', 'ozhotelstr', 'safamecomercial', '_felxtv_', 'dentsuadiye', 'tiyatroboyalikus', 'basaritelematik', 'ciplakayaklarkumpanyasi', 'mahmut_mirkelamm', 'tekincreativemedia', 'tasovabelediyesi', 'supra_hubs', 'mandalajans', 'ebcinege', 'adilyildirimyazar', 'asiyansanat', 'muharremakcadurak', 'donercibekirzade', 'sherwoodresorts', 'grandyaziciclubturban', 'birbuketmuge', 'bozcaadabld', 'dijitalsosyalmedyam', 'adel.turkiye', 'benim.odam', 'manolyaurkan', 'donerciorhan', 'akcoat_official', 'evimdeterapi', 'zettdekor', 'agrotar_turkey', 'pasinlerbld', 'tipografi', 'nurettinsonmez', 'tubabustun.official', '35likmeyhaneizmir', 'stilac', 'boyrazizolasyon', 'dentisteturkiye', 'altinayteknoloji', 'avbakidemirbas', 'diyeting', 'superfitshoes_turkey', 'diyetebasliyorumcom', 'pizzastationturkiye', 'adiyamanvalilik', 'cevrecienerjidernegi', 'novaarsaofficial', 'rubisaglik', 'denizbaysal_', 'drtubagunebak', 'rotekstekstil', 'burkaystagram', 'yenisehir.bel.tr', 'pelinnarint', 'partibutik', 'simalcerenn', 'buklatur', 'oxabrasive', 'marka.333', 'gulsjourney', 'gdz_edas', 'turyap', 'ugurakkafa', 'tiesfed', 'zabeyazgul', 'fordotosan', 'sevilayyilman', 'instakocaali', 'regispatent', 'lezzetborsasi', 'netafimturkiye', 'evreninmatematigi', 'seyhanlarmarket', 'relaxmodeofficial', 'ataberk.dogan', 'meliketatar', 'tohumgubre', 'ebebek', 'mabbelsatolye', 'hopiapp', 'tariszeytinyagi', 'tahsinhasoglu', 'longosphere', 'peymankuruyemis', 'trendylarahotel', 'pedasled', 'besiktasshipyard', 'puccito', 'malatyakultursosyal', 'sofosistanbul', 'devrimyakut', 'forevernewturkiye', 'valigokmencicek_', 'akbanksanat', 'isaleticom', 'veli.gok32', 'trio_deniz_official', 'mcengizbozkurt', 'istanbulalpplastik', 'pelinsutilki', 'iamemrecaliskan', 'ismailgeduz', 'selfycomtr', 'mstismakinalari', 'studio.oyunculari', 'nexuskuafor', 'cznburak', 'etidanismanlik', 'yildizgaz', 'bademlebuduk', 'sayginbaykaracom', 'onder.sayan', 'tiyatroumay', 'maraskaski', 'bebedorturkey', 'jumboturkiye', 'alacabld', 'bs_forklift', 'superfm', 'yagizkurtulusy', 'yasemoz88', 'thermexturkiye', 'polarisayakkabi', 'edremitbelediye', 'dr.yasirgozu', 'middelrestaurant', 'mtmofamily', 'yelizyesilmen', 'sumerpark_avm', 'sozerinsaatorhangazi', 'digitalaths', 'gulcinsant', 'endlessaksuluayakkabi', 'minisoturkey', 'aysatiyatro', 'artlineturkiye', 'montel.official', 'dizayngroup', 'asliafsaroglu', 'karefilm', 'moc_coffeeofficial', 'talyatasarimtr', 'arguvanbelediyesi', 'cengelofficial', 'ekulak', 'gokgunnec', 'arzumkoyuncu', 'fiyuutr', 'aynesgida', 'oyuncusendika', 'jehanbarbur', 'rafadantayfa', 'basf_tarim_tr', 'ardesenbelediyesi', 'kuirfest', 'olips_tr', 'muzegazhane', 'ondibilisim', 'gazihastanesi', 'gelinevi_tv', 'heyceyn', 'tekhnelogos', 'petadijital', 'gustogiyim', 'akasyaa_avm', 'arslangurerr', 'dr.turhanguldas', 'orcundalarslan', 'alldaycharm', 'mxnsturkey', 'talaycihan', 'kutuphane79', 'sevketcoruh', 'aliyerlikaya_', 'sacimsacinolsun', 'mhmtaktrk', 'jessicamayofficial', 'ceturturkiye', 'elifatalar', 'profdrmurataksoy', 'gastrogezgin', 'axisistanbulavm', 'petopytr', 'berkaytulumbaci', 'evdekolay', 'viteltr', 'merveozbeyofficial', 'no224creative', 'goksu_avm', 'baharakinci', 'mengenbelediyesi1414', 'batuconcept', 'ih_korkmaz', 'piculetstore', 'kadinozsavunma', 'borckabld', 'biryemekiste', 'evkokusu', 'maison_turkey', 'ceyhanbeltr', 'daricabelediyesi', 'korayaydintr', 'izmiretkinlikleri', 'labrandaephesusprincess', 'sanliurfabld', 'eczacibasisporkulubu', 'gulrizandic', 'mustafatokat77', 'iqaluminiumsystems', 'carpetartofficial', 'wcollection', 'kutu.baligi', 'gimatmarketlerzincirii', 'jalpersanofficial', 'luleburgazbld', 'paragrafmedya', 'ozgurcamurlu', 'ekimsanat', 'fixgross', 'konyafarukgulluoglu', 'yiyomfoodblogger', 'dorukyazilim', 'mantragastropub', 'yudumyag', 'kiralarsin', 'orhansef', 'merterenbulbul1', 'antaresavm', 'mutfakta_bebek_var', 'kaanvarli', 'finikebelediye', 'brotherturkey', 'justfeelingstudio', 'gezsenbatman', 'yapimboyle', 'quickchina', 'amazon.tr', 'ihealthsaglik', 'alicanaytekin', 'elegansgrupkagitcilik', 'elementlogistics', 'gumus_bahattin', 'bengikurtcebe', 'asyaalizaude', 'solargezi', 'fevzizirhlioglu', 'kozantepkunefe', 'kasapoglu', 'fourestcalticak', 'altinorumcek', 'arzuakbaszor', 'ahmettatlises', 'didemsoydan', 'acemmarket', 'idasosyalmedya', 'pertekbelediyesi', 'fixpacktr', 'feriyeistanbul', 'okanerertugrul', 'isikcelik.a.s', 'dpperfumum', 'nilaybale', 'fahrigediz', 'vanedremitbld', 'ebruuyucell', 'menopozveyasamtr', 'altekyapi', 'thermomixtr', 'bitaksi', 'antalyakultursanat', 'sergenozn', 'cemuzan', 'enka_insaat_official', 'socialbrandstr', 'alialtuntasbaskan', 'silivritvtr', 'vimerang', 'omurgedik', 'richmondnua', 'psikologmerkezi', 'divan_sarraf', 'baykushbaby', 'kademe.tr', 'motolojim', 'guven_ovun', 'argoalsancak', 'kelambazblog', 'grand_cevahir_hotel', 'aysebilgeselcuk', 'dilekcesur_', 'agoraavm_antalya', 'tugkangonultas', 'roubloff_turkey', 'prof.dr.halilalis', 'driclortr', 'tiyatroterminal', 'cartmancase', 'farkyaratanlarsabancivakfi', 'kosekahve', 'kadikoyboasahne', 'kdzereglitso', 'pepsiturkiye', 'estetikeskisehir', 'itsumijapon', 'dyt.omercanyorulmaz', 'kucukkulup_', 'tugrulkuyumculuk', 'eczozgurozel', 'merrellturkiye', 'rivadent_maltepe', 'gastronometro', 'seyhan_ekspres', 'minikseyler_', 'kasimpasask', 'otokiosktr', 'karasugold', 'mostphotos', 'swothospitality', 'kuff.yeldegirmeni', 'birceakalay', 'aynurtartan', 'brixtonturkiye', 'reklamcikafasi', 'utkucubukcuoglu', 'ucan_baba', 'rehberle', 'ntaslicay', 'dogukan_kilic']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Read the train-classification.csv file and create a mapping of usernames to categories\n",
    "category_mapping = []\n",
    "with open('test-classification-round3.dat', 'r') as dat_file:\n",
    "    for line in dat_file:\n",
    "        category_mapping.append(line.strip())\n",
    "\n",
    "print(category_mapping)\n",
    "\n",
    "# Step 1: Read the .jsonl file and process each line\n",
    "output_lines = []\n",
    "with open('processed_dataset-r3v7.jsonl', 'r') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        data = json.loads(line)\n",
    "        profile_data = data['profile']\n",
    "        if profile_data['username'] in category_mapping:\n",
    "            # Step 2: For each post, merge the profile data\n",
    "            for post in data['posts']:\n",
    "                post.update(profile_data)\n",
    "                output_lines.append(json.dumps(post))\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Write the flattened posts to a new .jsonl file\n",
    "with open('processed_dataset-r3v10-test.jsonl', 'w') as output_file:\n",
    "    for output_line in output_lines:\n",
    "        output_file.write(output_line + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
