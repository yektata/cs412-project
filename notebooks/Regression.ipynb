{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv6-85GZ-Udt",
        "outputId": "213070e4-ca17-4356-f24c-447c129d7cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrR3DEceEyxx"
      },
      "source": [
        "# Instagram Like Count Prediction CS 412 Project\n",
        "\n",
        "## Problem Overview\n",
        "The task involves predicting the number of likes an Instagram post will receive based on various features from both the post itself and the user's profile. This is structured as a regression problem where we predict a continuous numerical value (like count) for each post.\n",
        "\n",
        "## Data Processing Pipeline\n",
        "\n",
        "### 1. Feature Engineering\n",
        "We engineered several features from the raw data:\n",
        "\n",
        "#### Post-Level Features:\n",
        "- Comment count (log-transformed)\n",
        "- Caption length\n",
        "- Emoji count in caption\n",
        "- Hashtag count in caption\n",
        "- Temporal features (year, month, day, hour) from timestamp\n",
        "\n",
        "#### Profile-Level Features:\n",
        "- Follower count (log-transformed)\n",
        "- Following count (log-transformed)\n",
        "- Highlight reel count\n",
        "- Profile completeness metrics\n",
        "\n",
        "#### Engagement Metrics:\n",
        "- Engagement rate: (likes + comments) / followers\n",
        "- Comment-to-follower ratio\n",
        "- Following-to-follower ratio\n",
        "\n",
        "### 2. Data Preprocessing\n",
        "- Handled missing values with appropriate defaults\n",
        "- Applied log transformation to handle skewed distributions in metrics like follower count and comment count\n",
        "- Scaled numerical features using StandardScaler\n",
        "- Extracted and processed text features from captions and bio\n",
        "- Cleaned and normalized text data\n",
        "\n",
        "### 3. Model Development\n",
        "\n",
        "#### Model Selection\n",
        "We experimented with two powerful tree-based models:\n",
        "1. Random Forest Regressor\n",
        "   - Handles non-linear relationships\n",
        "   - Good with both numerical and categorical features\n",
        "   - Less prone to overfitting\n",
        "\n",
        "2. XGBoost Regressor\n",
        "   - Gradient boosting implementation\n",
        "   - Known for high performance in various tasks\n",
        "   - Efficient handling of sparse data\n",
        "\n",
        "#### Training Approach\n",
        "- Split data into training (80%) and validation (20%) sets\n",
        "- Trained both models with default parameters\n",
        "- Evaluated using Mean Squared Error (MSE) on validation set\n",
        "- Selected the best performing model for final predictions\n",
        "\n",
        "### 4. Prediction Pipeline\n",
        "1. Load and preprocess test data using the same pipeline as training\n",
        "2. Apply feature engineering steps\n",
        "3. Scale features using the fitted scaler\n",
        "4. Generate predictions using the best model\n",
        "5. Transform predictions back to original scale\n",
        "6. Round predictions to integers\n",
        "7. Save results in required format\n",
        "\n",
        "\n",
        "## Potential Improvements\n",
        "1. Feature engineering:\n",
        "   - Add more sophisticated text analysis\n",
        "   - Include image-based features if available\n",
        "   - Consider user engagement history\n",
        "\n",
        "2. Model enhancements:\n",
        "   - Fine-tune hyperparameters with more number of trials\n",
        "   - Experiment with ensemble methods\n",
        "   - Try deep learning approaches\n",
        "\n",
        "3. Data processing:\n",
        "   - Implement more sophisticated handling of outliers\n",
        "   - Add feature selection methods\n",
        "   - Consider time-based cross-validation\n",
        "\n",
        "## Conclusion\n",
        "Our approach focused on creating robust features and using reliable tree-based models for prediction. The pipeline is designed to be efficient and maintainable, with clear separation of concerns between data processing, model training, and prediction generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJMXbOUgEzXm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCC_L2WxARE4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd5Nj9CuKWtY",
        "outputId": "7d888b73-8c53-44a4-d417-7f802fbf1d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4ksEYJ4q4jy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0ew8MMYWCQ0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiCeFMk2q05W",
        "outputId": "772df9e2-d227-4520-bb67-aa55cab7e075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-01-12 19:36:39,579] A new study created in memory with name: no-name-a38738ba-2727-4d89-96ee-9beee87b46be\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Optimizing RandomForest...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-01-12 19:44:21,056] Trial 0 finished with value: 0.0022331472528910224 and parameters: {'n_estimators': 282, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.0022331472528910224.\n",
            "[I 2025-01-12 19:51:22,349] Trial 1 finished with value: 0.007038977227233935 and parameters: {'n_estimators': 429, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.0022331472528910224.\n",
            "[I 2025-01-12 20:02:57,723] Trial 2 finished with value: 0.0022998528458576706 and parameters: {'n_estimators': 476, 'max_depth': 16, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.0022331472528910224.\n",
            "[I 2025-01-12 20:12:18,144] Trial 3 finished with value: 0.002397880571353724 and parameters: {'n_estimators': 371, 'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.0022331472528910224.\n",
            "[I 2025-01-12 20:18:46,343] Trial 4 finished with value: 0.0025888674752862035 and parameters: {'n_estimators': 312, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.0022331472528910224.\n",
            "[I 2025-01-12 20:26:13,042] A new study created in memory with name: no-name-1c5f4fa0-0176-48e9-bb60-5c78421c2a9a\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest MSE: 0.0022\n",
            "\n",
            "Optimizing XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-01-12 20:26:32,376] Trial 0 finished with value: 0.002966840691203825 and parameters: {'n_estimators': 571, 'max_depth': 9, 'learning_rate': 0.18402231283944745, 'subsample': 0.8758857588695288, 'colsample_bytree': 0.8874321929620825, 'min_child_weight': 5}. Best is trial 0 with value: 0.002966840691203825.\n",
            "[I 2025-01-12 20:26:51,713] Trial 1 finished with value: 0.0030083868181762607 and parameters: {'n_estimators': 321, 'max_depth': 10, 'learning_rate': 0.012649650579138413, 'subsample': 0.8075286864638349, 'colsample_bytree': 0.9543357831918564, 'min_child_weight': 5}. Best is trial 0 with value: 0.002966840691203825.\n",
            "[I 2025-01-12 20:27:01,157] Trial 2 finished with value: 0.00231242525286502 and parameters: {'n_estimators': 319, 'max_depth': 7, 'learning_rate': 0.05542103859515586, 'subsample': 0.8712902499575725, 'colsample_bytree': 0.8739136660688364, 'min_child_weight': 1}. Best is trial 2 with value: 0.00231242525286502.\n",
            "[I 2025-01-12 20:27:07,198] Trial 3 finished with value: 0.0022764186833187034 and parameters: {'n_estimators': 469, 'max_depth': 6, 'learning_rate': 0.14714101245676187, 'subsample': 0.7786141083725342, 'colsample_bytree': 0.8436339998292943, 'min_child_weight': 1}. Best is trial 3 with value: 0.0022764186833187034.\n",
            "[I 2025-01-12 20:27:17,709] Trial 4 finished with value: 0.0020822109724285406 and parameters: {'n_estimators': 578, 'max_depth': 6, 'learning_rate': 0.07687176559631473, 'subsample': 0.864598890472713, 'colsample_bytree': 0.7895352660217848, 'min_child_weight': 2}. Best is trial 4 with value: 0.0020822109724285406.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost MSE: 0.0021\n",
            "Generating predictions...\n",
            "Predictions saved to prediction-regression-round3.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import emoji\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import optuna\n",
        "\n",
        "# Utility functions\n",
        "def extract_emojis_and_tags(text):\n",
        "    if not text:\n",
        "        return \"\", \"\", []\n",
        "    emojis = ''.join(char for char in text if char in emoji.EMOJI_DATA)\n",
        "    hashtags = re.findall(r\"#\\w+\", text)\n",
        "    clean_text = re.sub(r\"#\\w+\", \"\", text)\n",
        "    clean_text = ''.join(char for char in clean_text if char not in emoji.EMOJI_DATA)\n",
        "    return clean_text.strip(), emojis, hashtags\n",
        "\n",
        "def process_timestamp(timestamp):\n",
        "    try:\n",
        "        dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
        "        return dt.year, dt.month, dt.day, dt.hour\n",
        "    except (ValueError, TypeError):\n",
        "        return None, None, None, None\n",
        "\n",
        "def to_log_scale(value):\n",
        "    try:\n",
        "        value = float(value if value is not None else 0)\n",
        "        return np.log10(1 + value) if value > 0 else 0\n",
        "    except (ValueError, TypeError):\n",
        "        return 0\n",
        "\n",
        "def inverse_log_transform(y_pred):\n",
        "    return (10 ** y_pred) - 1\n",
        "\n",
        "def safe_get(dictionary, key, default=0):\n",
        "    \"\"\"Safely get a value from a dictionary, handling None values\"\"\"\n",
        "    value = dictionary.get(key, default)\n",
        "    return value if value is not None else default\n",
        "\n",
        "class InstagramEngagementPredictor:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.best_model = None\n",
        "\n",
        "    def extract_features(self, record, is_training=True):\n",
        "        features = []\n",
        "        labels = []\n",
        "        post_ids = []\n",
        "\n",
        "        posts = record.get('posts', []) if is_training else [record]\n",
        "        profile = record.get('profile', {}) if record.get('profile') is not None else {}\n",
        "\n",
        "        for post in posts:\n",
        "            # Get basic counts with safe handling of None values\n",
        "            like_count = safe_get(post, 'like_count', 0)\n",
        "            comment_count = safe_get(post, 'comments_count', 0)\n",
        "            follower_count = safe_get(profile, 'follower_count', 1)  # Use 1 as default to avoid division by zero\n",
        "            following_count = safe_get(profile, 'following_count', 0)\n",
        "\n",
        "            # Process timestamp\n",
        "            year, month, day, hour = process_timestamp(safe_get(post, 'timestamp'))\n",
        "\n",
        "            # Process caption\n",
        "            caption = safe_get(post, 'caption', '')\n",
        "            clean_caption, emojis, hashtags = extract_emojis_and_tags(caption)\n",
        "\n",
        "            # Calculate engagement metrics\n",
        "            engagement_rate = (float(like_count) + float(comment_count)) / float(follower_count)\n",
        "            comment_to_follower_ratio = float(comment_count) / float(follower_count)\n",
        "            following_to_follower_ratio = float(following_count) / float(follower_count)\n",
        "\n",
        "            feature_dict = {\n",
        "                'engagement_rate': engagement_rate,\n",
        "                'follower_count': to_log_scale(follower_count),\n",
        "                'comments_count': to_log_scale(comment_count),\n",
        "                'caption_length': len(clean_caption),\n",
        "                'emoji_count': len(emojis),\n",
        "                'hashtag_count': len(hashtags),\n",
        "                'following_count': to_log_scale(following_count),\n",
        "                'highlight_reel_count': safe_get(profile, 'highlight_reel_count', 0),\n",
        "                'day': day if day is not None else 1,\n",
        "                'hour': hour if hour is not None else 0,\n",
        "                'comment_to_follower_ratio': comment_to_follower_ratio,\n",
        "                'following_to_follower_ratio': following_to_follower_ratio\n",
        "            }\n",
        "\n",
        "            features.append(feature_dict)\n",
        "            if is_training:\n",
        "                labels.append(to_log_scale(like_count))\n",
        "            post_ids.append(safe_get(post, 'id'))\n",
        "\n",
        "        return features, labels, post_ids\n",
        "\n",
        "    def load_data(self, file_path, is_training=True):\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "        all_post_ids = []\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                record = json.loads(line)\n",
        "                features, labels, post_ids = self.extract_features(record, is_training)\n",
        "                features_list.extend(features)\n",
        "                if is_training:\n",
        "                    labels_list.extend(labels)\n",
        "                all_post_ids.extend(post_ids)\n",
        "\n",
        "        X = pd.DataFrame(features_list)\n",
        "        X = X.fillna(0)  # Fill any remaining NaN values\n",
        "        if is_training:\n",
        "            return X, np.array(labels_list)\n",
        "        return X, all_post_ids\n",
        "\n",
        "    def train_and_evaluate(self, training_file, n_trials=5):\n",
        "        # Load and prepare training data\n",
        "        print(\"Loading training data...\")\n",
        "        X, y = self.load_data(training_file)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Define model configurations\n",
        "        models = {\n",
        "            'RandomForest': {\n",
        "                'model': RandomForestRegressor,\n",
        "                'params': {\n",
        "                    'n_estimators': (200, 600),\n",
        "                    'max_depth': (5, 20),\n",
        "                    'min_samples_split': (2, 8),\n",
        "                    'min_samples_leaf': (1, 4)\n",
        "                }\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                'model': XGBRegressor,\n",
        "                'params': {\n",
        "                    'n_estimators': (200, 600),\n",
        "                    'max_depth': (4, 10),\n",
        "                    'learning_rate': (0.01, 0.2),\n",
        "                    'subsample': (0.7, 1.0),\n",
        "                    'colsample_bytree': (0.7, 1.0),\n",
        "                    'min_child_weight': (1, 7)\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        best_score = float('inf')\n",
        "\n",
        "        for model_name, config in models.items():\n",
        "            print(f\"\\nOptimizing {model_name}...\")\n",
        "\n",
        "            def objective(trial):\n",
        "                params = {\n",
        "                    name: (trial.suggest_int if isinstance(range, tuple) and isinstance(range[0], int)\n",
        "                          else trial.suggest_float)(name, *range)\n",
        "                    for name, range in config['params'].items()\n",
        "                }\n",
        "                params['random_state'] = 42\n",
        "\n",
        "                model = config['model'](**params)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_val)\n",
        "                return mean_squared_error(y_val, y_pred)\n",
        "\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "            # Train model with best parameters\n",
        "            model = config['model'](**study.best_params, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            y_pred = model.predict(X_val)\n",
        "            mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "            print(f\"{model_name} MSE: {mse:.4f}\")\n",
        "\n",
        "            if mse < best_score:\n",
        "                best_score = mse\n",
        "                self.best_model = model\n",
        "\n",
        "    def predict(self, test_file, output_file):\n",
        "        print(\"Generating predictions...\")\n",
        "        X_test, post_ids = self.load_data(test_file, is_training=False)\n",
        "\n",
        "        # Scale features\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        # Make predictions\n",
        "        log_predictions = self.best_model.predict(X_test_scaled)\n",
        "        predictions = inverse_log_transform(log_predictions)\n",
        "        predictions = np.maximum(0, predictions)\n",
        "\n",
        "        # Save predictions\n",
        "        with open(output_file, 'w') as f:\n",
        "            for post_id, pred in zip(post_ids, predictions):\n",
        "                f.write(json.dumps([post_id, int(round(pred))]) + '\\n')\n",
        "\n",
        "        print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "def main():\n",
        "    # File paths\n",
        "    training_file = \"/content/drive/MyDrive/ColabNotebooks/CS-412_ML-Project/dataset/training-dataset.jsonl\"\n",
        "    test_file = \"/content/drive/MyDrive/ColabNotebooks/CS-412_ML-Project/dataset/test-regression-round3.jsonl\"\n",
        "    predictions_output = \"prediction-regression-round3.jsonl\"\n",
        "\n",
        "    # Initialize and run pipeline\n",
        "    predictor = InstagramEngagementPredictor()\n",
        "    predictor.train_and_evaluate(training_file)\n",
        "    predictor.predict(test_file, predictions_output)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v7dMRx0cW1F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T26oeTlMdAUC",
        "outputId": "13a7ed9a-3195-4b3c-ece8-b526b50885be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "\n",
            "Training RandomForest...\n",
            "RandomForest MSE: 0.0020\n",
            "New best model: RandomForest\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost MSE: 0.0030\n",
            "Generating predictions...\n",
            "Predictions saved to prediction-regression-round3.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import emoji\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Utility functions\n",
        "def extract_emojis_and_tags(text):\n",
        "    if not text:\n",
        "        return \"\", \"\", []\n",
        "    emojis = ''.join(char for char in text if char in emoji.EMOJI_DATA)\n",
        "    hashtags = re.findall(r\"#\\w+\", text)\n",
        "    clean_text = re.sub(r\"#\\w+\", \"\", text)\n",
        "    clean_text = ''.join(char for char in clean_text if char not in emoji.EMOJI_DATA)\n",
        "    return clean_text.strip(), emojis, hashtags\n",
        "\n",
        "def process_timestamp(timestamp):\n",
        "    try:\n",
        "        dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
        "        return dt.year, dt.month, dt.day, dt.hour\n",
        "    except (ValueError, TypeError):\n",
        "        return None, None, None, None\n",
        "\n",
        "def to_log_scale(value):\n",
        "    try:\n",
        "        value = float(value if value is not None else 0)\n",
        "        return np.log10(1 + value) if value > 0 else 0\n",
        "    except (ValueError, TypeError):\n",
        "        return 0\n",
        "\n",
        "def inverse_log_transform(y_pred):\n",
        "    return (10 ** y_pred) - 1\n",
        "\n",
        "def safe_get(dictionary, key, default=0):\n",
        "    \"\"\"Safely get a value from a dictionary, handling None values\"\"\"\n",
        "    value = dictionary.get(key, default)\n",
        "    return value if value is not None else default\n",
        "\n",
        "class InstagramEngagementPredictor:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.best_model = None\n",
        "\n",
        "    def extract_features(self, record, is_training=True):\n",
        "        features = []\n",
        "        labels = []\n",
        "        post_ids = []\n",
        "\n",
        "        posts = record.get('posts', []) if is_training else [record]\n",
        "        profile = record.get('profile', {}) if record.get('profile') is not None else {}\n",
        "\n",
        "        for post in posts:\n",
        "            # Get basic counts with safe handling of None values\n",
        "            like_count = safe_get(post, 'like_count', 0)\n",
        "            comment_count = safe_get(post, 'comments_count', 0)\n",
        "            follower_count = safe_get(profile, 'follower_count', 1)\n",
        "            following_count = safe_get(profile, 'following_count', 0)\n",
        "\n",
        "            # Process timestamp\n",
        "            year, month, day, hour = process_timestamp(safe_get(post, 'timestamp'))\n",
        "\n",
        "            # Process caption\n",
        "            caption = safe_get(post, 'caption', '')\n",
        "            clean_caption, emojis, hashtags = extract_emojis_and_tags(caption)\n",
        "\n",
        "            # Calculate engagement metrics\n",
        "            engagement_rate = (float(like_count) + float(comment_count)) / float(follower_count)\n",
        "            comment_to_follower_ratio = float(comment_count) / float(follower_count)\n",
        "            following_to_follower_ratio = float(following_count) / float(follower_count)\n",
        "\n",
        "            feature_dict = {\n",
        "                'engagement_rate': engagement_rate,\n",
        "                'follower_count': to_log_scale(follower_count),\n",
        "                'comments_count': to_log_scale(comment_count),\n",
        "                'caption_length': len(clean_caption),\n",
        "                'emoji_count': len(emojis),\n",
        "                'hashtag_count': len(hashtags),\n",
        "                'following_count': to_log_scale(following_count),\n",
        "                'highlight_reel_count': safe_get(profile, 'highlight_reel_count', 0),\n",
        "                'day': day if day is not None else 1,\n",
        "                'hour': hour if hour is not None else 0,\n",
        "                'comment_to_follower_ratio': comment_to_follower_ratio,\n",
        "                'following_to_follower_ratio': following_to_follower_ratio\n",
        "            }\n",
        "\n",
        "            features.append(feature_dict)\n",
        "            if is_training:\n",
        "                labels.append(to_log_scale(like_count))\n",
        "            post_ids.append(safe_get(post, 'id'))\n",
        "\n",
        "        return features, labels, post_ids\n",
        "\n",
        "    def load_data(self, file_path, is_training=True):\n",
        "        features_list = []\n",
        "        labels_list = []\n",
        "        all_post_ids = []\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                record = json.loads(line)\n",
        "                features, labels, post_ids = self.extract_features(record, is_training)\n",
        "                features_list.extend(features)\n",
        "                if is_training:\n",
        "                    labels_list.extend(labels)\n",
        "                all_post_ids.extend(post_ids)\n",
        "\n",
        "        X = pd.DataFrame(features_list)\n",
        "        X = X.fillna(0)\n",
        "        if is_training:\n",
        "            return X, np.array(labels_list)\n",
        "        return X, all_post_ids\n",
        "\n",
        "    def train_and_evaluate(self, training_file):\n",
        "        # Load and prepare training data\n",
        "        print(\"Loading training data...\")\n",
        "        X, y = self.load_data(training_file)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Define and train models with default parameters\n",
        "        models = {\n",
        "            'RandomForest': RandomForestRegressor(n_estimators=200, random_state=42),\n",
        "            'XGBoost': XGBRegressor(n_estimators=200, random_state=42)\n",
        "        }\n",
        "\n",
        "        best_score = float('inf')\n",
        "\n",
        "        # Train and evaluate each model\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_val)\n",
        "            mse = mean_squared_error(y_val, y_pred)\n",
        "            print(f\"{name} MSE: {mse:.4f}\")\n",
        "\n",
        "            if mse < best_score:\n",
        "                best_score = mse\n",
        "                self.best_model = model\n",
        "                print(f\"New best model: {name}\")\n",
        "\n",
        "    def predict(self, test_file, output_file):\n",
        "        print(\"Generating predictions...\")\n",
        "        X_test, post_ids = self.load_data(test_file, is_training=False)\n",
        "\n",
        "        # Scale features\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        # Make predictions\n",
        "        log_predictions = self.best_model.predict(X_test_scaled)\n",
        "        predictions = inverse_log_transform(log_predictions)\n",
        "        predictions = np.maximum(0, predictions)\n",
        "\n",
        "        # Save predictions\n",
        "        with open(output_file, 'w') as f:\n",
        "            for post_id, pred in zip(post_ids, predictions):\n",
        "                f.write(json.dumps([post_id, int(round(pred))]) + '\\n')\n",
        "\n",
        "        print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "def main():\n",
        "    # File paths\n",
        "    training_file = \"/content/drive/MyDrive/ColabNotebooks/CS-412_ML-Project/dataset/training-dataset.jsonl\"\n",
        "    test_file = \"/content/drive/MyDrive/ColabNotebooks/CS-412_ML-Project/dataset/test-regression-round3.json\"\n",
        "    predictions_output = \"prediction-regression-round3.jsonl\"\n",
        "\n",
        "    # Initialize and run pipeline\n",
        "    predictor = InstagramEngagementPredictor()\n",
        "    predictor.train_and_evaluate(training_file)\n",
        "    predictor.predict(test_file, predictions_output)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
